{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics.pairwise import pairwise_distances_argmin_min\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "from torch_geometric.nn import global_mean_pool, global_add_pool, global_max_pool\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "from torch_geometric.nn import GCNConv, GATConv, SimpleConv\n",
    "from torch_geometric.transforms import Constant\n",
    "import pickle\n",
    "Memo = {}\n",
    "device = torch.device(\"cuda\") # Set device (use GPU if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_memo(memo, filename = 'records50.dat'):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(memo, f)\n",
    "\n",
    "def load_memo(filename = 'records50.dat'):\n",
    "    try:\n",
    "        with open(filename, 'rb') as f:\n",
    "            memo = pickle.load(f)\n",
    "            return memo\n",
    "    except FileNotFoundError:\n",
    "        print(\"Memo file not found. Returning an empty dictionary.\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomGraphDataset(Dataset):\n",
    "    def __init__(self, root, split='train', transform=None):\n",
    "         # Initialize the dataset with the root directory, split, and optional transform\n",
    "        self.root = root\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.graph_files = []\n",
    "\n",
    "        classes = os.listdir(root) # Get the list of classes (subdirectories) in the root directory\n",
    "\n",
    "        # Iterate through each class\n",
    "        for class_folder in classes:\n",
    "            class_path = os.path.join(root, class_folder)\n",
    "            \n",
    "            # Check if the class folder is a directory\n",
    "            if os.path.isdir(class_path):\n",
    "                # Create the path to the split folder (train, test, etc.) within the class folder\n",
    "                split_folder = os.path.join(class_path, split)\n",
    "                \n",
    "                # Get the list of graph files (with the .gph extension) in the split folder\n",
    "                graph_files = [os.path.join(split_folder, f) for f in os.listdir(split_folder) if f.endswith('.gph')]\n",
    "                \n",
    "                # Extend the list of graph files in the dataset with the current class's graph files\n",
    "                self.graph_files.extend(graph_files)\n",
    "\n",
    "    def __len__(self): # Return the total number of graph files in the dataset\n",
    "        return len(self.graph_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the file path of the graph file at the specified index\n",
    "        graph_file = self.graph_files[idx]\n",
    "        \n",
    "        # Load the graph data from the file using torch\n",
    "        data = torch.load(graph_file)\n",
    "        \n",
    "        # Add ground truth label based on class (assuming class names in the file paths)\n",
    "        class_label = None\n",
    "        for i, class_name in enumerate(['bathtub', 'bed', 'chair', 'desk', 'dresser', 'monitor', 'night', 'sofa', 'table', 'toilet']):\n",
    "            if class_name in graph_file:\n",
    "                class_label = i\n",
    "                break\n",
    "\n",
    "        # Check if class_label is still None, indicating an unknown class\n",
    "        if class_label is None:\n",
    "            raise ValueError(f\"Unknown class for file: {graph_file}\")\n",
    "\n",
    "        # Set the ground truth label as a tensor in the 'y' attribute of the graph data\n",
    "        data.y = torch.tensor([class_label], dtype=torch.long)\n",
    "\n",
    "        # Apply the transform if provided\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "# Load the graph data using the CustomGraphDataset\n",
    "def load_graph_data(filepath):\n",
    "    dataset = CustomGraphDataset(filepath, transform=None)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genetic Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_size = 10\n",
    "mutation_rate = 0.2\n",
    "tournament_size = 4\n",
    "num_gens = 40\n",
    "root_path = '3DGraphz7'\n",
    "epochs = 50\n",
    "\n",
    "hyperparameters = {\n",
    "    \"learning_rate\": [0.001, 0.005, 0.01, 0.05, 0.1],\n",
    "    \"batch_size\": [4, 8, 16, 32, 64, 128],\n",
    "    \"layer_type\": [GCNConv, GATConv],  # The type of GNN layers to use\n",
    "    \"hidden\": [0, 4, 8, 16, 32, 64, 128],  # The number of hidden units in each layer\n",
    "    \"weight_decay\": [1e-5, 5e-4, 1e-3],  # The rate at which weights decay for regularization\n",
    "    \"activation\": [torch.relu, torch.tanh, torch.sigmoid],  # The activation function to use in the hidden layers\n",
    "    \"pooling\": [global_add_pool, global_mean_pool, global_max_pool]\n",
    "}\n",
    "\n",
    "Encoding = [\"layer_type\", \"hidden\", \"hidden\", \"hidden\", \"hidden\", \"activation\", \"pooling\", \"learning_rate\", \"batch_size\", \"weight_decay\"]\n",
    "Range_of_parameters = [len(hyperparameters[y]) for y in Encoding]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_population(n, m):\n",
    "    permutations_list = []\n",
    "    while len(permutations_list) < n:\n",
    "        permutation = []\n",
    "        for i in range(m):\n",
    "            permutation.append(random.choice(range(Range_of_parameters[i])))\n",
    "        if permutation not in permutations_list:\n",
    "            permutations_list.append(permutation)\n",
    "    return permutations_list\n",
    "\n",
    "def n_crossover(parent1, parent2):\n",
    "    offspring1 = parent1.copy()\n",
    "    offspring2 = parent2.copy()\n",
    "    for i in range(len(parent1)):\n",
    "        if i % 2 == 0:\n",
    "            offspring1[i] = parent1[i]\n",
    "            offspring2[i] = parent2[i]\n",
    "        else:\n",
    "            offspring1[i] = parent2[i]\n",
    "            offspring2[i] = parent1[i]\n",
    "    return [offspring1, offspring2]\n",
    "\n",
    "def mutate(person, perc):\n",
    "    mutated_person = person.copy()\n",
    "    while random.random() < perc:\n",
    "        i = random.randint(0,len(mutated_person) - 1)\n",
    "        inc = random.choice([1, -1])\n",
    "        mutated_person[i] = (mutated_person[i] + inc) % Range_of_parameters[i]\n",
    "    return mutated_person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness_based_selection(population,fitness_function ,crossover_function, mutation_function, mutation_rate, allow_parents = True):\n",
    "    fitness_values = [fitness_function(individual) for individual in population]\n",
    "    new_population = []\n",
    "    \n",
    "    if allow_parents == True:\n",
    "        new_population = population.copy()\n",
    "    \n",
    "    for i in range(len(population)):\n",
    "        offsprings = []\n",
    "        index = 0\n",
    "        print(offsprings)\n",
    "        while(len(offsprings) < 2):\n",
    "            parent1 = random.choices(population, weights=fitness_values)[0]\n",
    "            parent2 = random.choices(population, weights=fitness_values)[0]\n",
    "\n",
    "            offsprings = crossover_function(parent1, parent2)\n",
    "            index += 1\n",
    "            if index >=100:\n",
    "                break\n",
    "        for offspring in offsprings:\n",
    "            new_population += [mutation_function(offspring, mutation_rate)]\n",
    "        \n",
    "    return new_population\n",
    "\n",
    "def ranked_based_selection(population, fitness_function, crossover_function, mutation_function, mutation_rate, allow_parents=True):\n",
    "    sorted_population = sorted(population, key=lambda x: fitness_function(x))\n",
    "    population_size = len(population)\n",
    "\n",
    "    selection_weights = [i for i in range(1, population_size + 1)]\n",
    "    \n",
    "    new_population = []\n",
    "    if allow_parents:\n",
    "        new_population = sorted_population.copy()\n",
    "    \n",
    "    for i in range(len(population)):\n",
    "        \n",
    "        offsprings = []\n",
    "        index = 0\n",
    "        while(len(offsprings) < 2):\n",
    "            parent1 = random.choices(sorted_population, weights=selection_weights)[0]\n",
    "            parent2 = random.choices(sorted_population, weights=selection_weights)[0]\n",
    "\n",
    "            offsprings = crossover_function(parent1, parent2)\n",
    "            index += 1\n",
    "            if index >=100:\n",
    "                break\n",
    "        for offspring in offsprings:\n",
    "            new_population += [mutation_function(offspring, mutation_rate)]\n",
    "        \n",
    "    return new_population\n",
    "\n",
    "def tournament_ranked_based_selection(population,fitness_function, crossover_function, mutation_function, mutation_rate, tournament_size=2, allow_parents=True):\n",
    "    new_population = []\n",
    "\n",
    "    if allow_parents:\n",
    "        new_population = population.copy()\n",
    "   \n",
    "    for i in range(len(population)):\n",
    "\n",
    "        offsprings = []\n",
    "        index = 0\n",
    "        while(len(offsprings) < 2):\n",
    "            tournament_individuals = random.sample(population, tournament_size)\n",
    "            sorted_tournament = sorted(tournament_individuals, key=lambda x: fitness_function(x))\n",
    "\n",
    "            \n",
    "            selection_weights = [i for i in range(1, tournament_size + 1)]\n",
    "\n",
    "            parent1 = random.choices(sorted_tournament, weights=selection_weights)[0]\n",
    "            parent2 = random.choices(sorted_tournament, weights=selection_weights)[0]\n",
    "\n",
    "            offsprings = crossover_function(parent1, parent2)\n",
    "            index += 1\n",
    "            if index >=100:\n",
    "                break\n",
    "        \n",
    "        for offspring in offsprings:\n",
    "            new_population += [mutation_function(offspring, mutation_rate)]\n",
    "        \n",
    "    \n",
    "    return new_population\n",
    "\n",
    "def tournament_fitness_based_selection(population,fitness_function, crossover_function, mutation_function, mutation_rate, allow_parents=True):\n",
    "    new_population = []\n",
    "    if allow_parents:\n",
    "        new_population = population.copy()\n",
    "    \n",
    "    for i in range(len(population)):\n",
    "        offsprings = []\n",
    "        index = 0\n",
    "        while(len(offsprings) < 2):\n",
    "            tournament_individuals = random.sample(population, tournament_size)\n",
    "            tournament_fitness_values = [fitness_function(individual) for individual in tournament_individuals]\n",
    "            \n",
    "            parent1 = random.choices(tournament_individuals, weights=tournament_fitness_values)[0]\n",
    "            parent2 = random.choices(tournament_individuals, weights=tournament_fitness_values)[0]\n",
    "\n",
    "            offsprings = crossover_function(parent1, parent2)\n",
    "            index += 1\n",
    "            if index >=100:\n",
    "                break\n",
    "\n",
    "        for offspring in offsprings:\n",
    "            new_population += [mutation_function(offspring, mutation_rate)]\n",
    "        \n",
    "    return new_population\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argsort(seq):\n",
    "    return sorted(range(len(seq)), key=seq.__getitem__)[::-1]\n",
    "\n",
    "def truncation(population,fitness_function, npop):\n",
    "    fitness_values = [fitness_function(individual) for individual in population]\n",
    "    indexes = argsort(fitness_values)[:npop]\n",
    "    selected_population = [population[index] for index in indexes]\n",
    "    return selected_population\n",
    "\n",
    "def binary_tournament(population,fitness_function, npop):\n",
    "    retained_population = []\n",
    "    while len(retained_population) < npop:\n",
    "        ind1, ind2 = random.sample(population, 2)\n",
    "        winner = ind1 if fitness_function(ind1) > fitness_function(ind2) else ind2\n",
    "        retained_population.append(winner)\n",
    "    \n",
    "    return retained_population\n",
    "\n",
    "def genetic_algorithm(population, fitness_function, parent_selection_function = fitness_based_selection, survival_function = truncation, crossover_function = n_crossover, mutation_function = mutate, mutation_rate = 0.2, allow_parents = True):\n",
    "    new_population = parent_selection_function(population, fitness_function, crossover_function, mutation_function, mutation_rate, allow_parents)\n",
    "    return survival_function(new_population, fitness_function, len(population))\n",
    "\n",
    "def genetic_algorithm_n_gens(population,fitness_function, parent_selection_function = fitness_based_selection, survival_function = truncation, crossover_function = n_crossover, mutation_function = mutate, mutation_rate = 0.2, allow_parents = True,n_gens = num_gens, plot = True, display_last_gen = True):\n",
    "    \n",
    "    n = len(population)\n",
    "    \n",
    "    fitness_scores = [fitness_function(individual) for individual in population]\n",
    "    best_scores = [sorted(fitness_scores)[-1]]\n",
    "    average_scores = [sum(fitness_scores) / len(fitness_scores)]\n",
    "    \n",
    "    new_population = population.copy()\n",
    "    \n",
    "    title = f'{parent_selection_function.__name__} + {survival_function.__name__}'\n",
    "    for i in range(n_gens):\n",
    "        print(f\"Gen {i}: \")\n",
    "        new_population = parent_selection_function(new_population, fitness_function, crossover_function, mutation_function, mutation_rate, allow_parents)\n",
    "        new_population = survival_function(new_population, fitness_function, n)\n",
    "        fitness_scores = [fitness_function(individual) for individual in new_population]\n",
    "        best_scores += [sorted(fitness_scores)[-1]]\n",
    "        average_scores += [sum(fitness_scores) / len(fitness_scores) ]\n",
    "    \n",
    "    if plot == True:\n",
    "        # Plotting the data\n",
    "\n",
    "\n",
    "        plt.plot(best_scores, color='blue', label='Best Score')\n",
    "        plt.plot(average_scores, color='red', label='Average Score')\n",
    "        plt.xlabel('Generation')\n",
    "        plt.ylabel('Score')\n",
    "        plt.legend()\n",
    "        plt.title(f'Optimizing {fitness_function.__name__} with {parent_selection_function.__name__} + {survival_function.__name__}')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    if display_last_gen == True:\n",
    "        print(f'Generation: {n_gens} with {parent_selection_function.__name__} and {survival_function.__name__}')\n",
    "        for i in range(population_size):\n",
    "            print(f'{i+1}. Path = {new_population[i]}, Score = {fitness_function(new_population[i]):.4f}')\n",
    "    save_memo(Memo, 'records50.dat')\n",
    "    return title, best_scores, average_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNModelEA0(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim = [16], activation_func = torch.relu, layer = GCNConv, pooling = global_mean_pool):\n",
    "        super(GNNModelEA0, self).__init__()\n",
    "        # Graph convolutional layers\n",
    "        self.first = layer(input_dim, output_dim)\n",
    "\n",
    "        self.activation_func = activation_func\n",
    "        self.pooling = pooling\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        \n",
    "        # Apply the first graph convolutional layer followed by ReLU activation\n",
    "        x = self.first(x, edge_index)\n",
    "        x = self.activation_func(x)\n",
    "\n",
    "        # Global mean pooling over the nodes in each graph in the batch\n",
    "        x = self.pooling(x, batch)\n",
    "\n",
    "        # Apply log_softmax for classification\n",
    "        return torch.log_softmax(x, dim=1)\n",
    "\n",
    "class GNNModelEA1(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim = [16], activation_func = torch.relu, layer = GCNConv, pooling = global_mean_pool):\n",
    "        super(GNNModelEA1, self).__init__()\n",
    "        # Graph convolutional layers\n",
    "        self.first = layer(input_dim, hidden_dim[0])\n",
    "        self.hidden = None\n",
    "        self.out = layer(hidden_dim[0], output_dim)\n",
    "\n",
    "        self.activation_func = activation_func\n",
    "        self.pooling = pooling\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        \n",
    "        # Apply the first graph convolutional layer followed by ReLU activation\n",
    "        x = self.first(x, edge_index)\n",
    "        x = self.activation_func(x)\n",
    "\n",
    "        # Apply the third graph convolutional layer\n",
    "        x = self.out(x, edge_index)\n",
    "\n",
    "        # Global mean pooling over the nodes in each graph in the batch\n",
    "        x = self.pooling(x, batch)\n",
    "\n",
    "        # Apply log_softmax for classification\n",
    "        return torch.log_softmax(x, dim=1)\n",
    "    \n",
    "class GNNModelEA2(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim = [16, 16], activation_func = torch.relu, layer = GCNConv, pooling = global_mean_pool):\n",
    "        super(GNNModelEA2, self).__init__()\n",
    "        # Graph convolutional layers\n",
    "        self.first = layer(input_dim, hidden_dim[0])\n",
    "        self.hidden = layer(hidden_dim[0], hidden_dim[1])\n",
    "        self.out = layer(hidden_dim[1], output_dim)\n",
    "\n",
    "        self.activation_func = activation_func\n",
    "        self.pooling = pooling\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        \n",
    "        # Apply the first graph convolutional layer followed by ReLU activation\n",
    "        x = self.first(x, edge_index)\n",
    "        x = self.activation_func(x)\n",
    "\n",
    "        # Apply the second graph convolutional layer followed by ReLU activation\n",
    "        x = self.hidden(x, edge_index)\n",
    "        x = self.activation_func(x)\n",
    "\n",
    "        # Apply the third graph convolutional layer\n",
    "        x = self.out(x, edge_index)\n",
    "\n",
    "        # Global mean pooling over the nodes in each graph in the batch\n",
    "        x = self.pooling(x, batch)\n",
    "\n",
    "        # Apply log_softmax for classification\n",
    "        return torch.log_softmax(x, dim=1)\n",
    "\n",
    "class GNNModelEA3(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim = [16, 16, 16], activation_func = torch.relu, layer = GCNConv, pooling = global_mean_pool):\n",
    "        super(GNNModelEA3, self).__init__()\n",
    "        # Graph convolutional layers\n",
    "        self.first = layer(input_dim, hidden_dim[0])\n",
    "        self.hidden = layer(hidden_dim[0], hidden_dim[1])\n",
    "        self.hidden2 = layer(hidden_dim[1], hidden_dim[2])\n",
    "        self.out = layer(hidden_dim[2], output_dim)\n",
    "\n",
    "        self.activation_func = activation_func\n",
    "        self.pooling = pooling\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        \n",
    "        # Apply the first graph convolutional layer followed by ReLU activation\n",
    "        x = self.first(x, edge_index)\n",
    "        x = self.activation_func(x)\n",
    "\n",
    "        # Apply the second graph convolutional layer followed by ReLU activation\n",
    "        x = self.hidden(x, edge_index)\n",
    "        x = self.activation_func(x)\n",
    "\n",
    "        x = self.hidden2(x, edge_index)\n",
    "        x = self.activation_func(x)\n",
    "\n",
    "        # Apply the third graph convolutional layer\n",
    "        x = self.out(x, edge_index)\n",
    "\n",
    "        # Global mean pooling over the nodes in each graph in the batch\n",
    "        x = self.pooling(x, batch)\n",
    "\n",
    "        # Apply log_softmax for classification\n",
    "        return torch.log_softmax(x, dim=1)\n",
    "    \n",
    "class GNNModelEA4(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim = [16, 16, 16, 16], activation_func = torch.relu, layer = GCNConv, pooling = global_mean_pool):\n",
    "        super(GNNModelEA4, self).__init__()\n",
    "        # Graph convolutional layers\n",
    "        self.first = layer(input_dim, hidden_dim[0])\n",
    "        self.hidden = layer(hidden_dim[0], hidden_dim[1])\n",
    "        self.hidden2 = layer(hidden_dim[1], hidden_dim[2])\n",
    "        self.hidden3 = layer(hidden_dim[2], hidden_dim[3])\n",
    "        self.out = layer(hidden_dim[3], output_dim)\n",
    "\n",
    "        self.activation_func = activation_func\n",
    "        self.pooling = pooling\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        \n",
    "        # Apply the first graph convolutional layer followed by ReLU activation\n",
    "        x = self.first(x, edge_index)\n",
    "        x = self.activation_func(x)\n",
    "\n",
    "        # Apply the second graph convolutional layer followed by ReLU activation\n",
    "        x = self.hidden(x, edge_index)\n",
    "        x = self.activation_func(x)\n",
    "\n",
    "        x = self.hidden2(x, edge_index)\n",
    "        x = self.activation_func(x)\n",
    "\n",
    "        x = self.hidden3(x, edge_index)\n",
    "        x = self.activation_func(x)\n",
    "\n",
    "        # Apply the third graph convolutional layer\n",
    "        x = self.out(x, edge_index)\n",
    "\n",
    "        # Global mean pooling over the nodes in each graph in the batch\n",
    "        x = self.pooling(x, batch)\n",
    "\n",
    "        # Apply log_softmax for classification\n",
    "        return torch.log_softmax(x, dim=1)\n",
    "\n",
    "# Train the GNN model\n",
    "def train(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    for data in train_loader:\n",
    "        data = data.to(device) \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        _, predicted = output.max(1)\n",
    "        total_train += data.y.size(0)\n",
    "        correct_train += predicted.eq(data.y).sum().item()\n",
    "\n",
    "    accuracy_train = correct_train / total_train\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "    return accuracy_train, average_loss\n",
    "\n",
    "# Evaluate the GNN model\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct_eval = 0\n",
    "    total_eval = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device) \n",
    "            output = model(data)\n",
    "            loss = criterion(output, data.y)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, predicted = output.max(1)\n",
    "            total_eval += data.y.size(0)\n",
    "            correct_eval += predicted.eq(data.y).sum().item()\n",
    "\n",
    "    accuracy_eval = correct_eval / total_eval\n",
    "    average_loss = total_loss / len(loader)\n",
    "    return accuracy_eval, average_loss\n",
    "\n",
    "# Main function to train and evaluate the GNN model\n",
    "def fitness_function(person, epochs = epochs):\n",
    "    person = tuple(person)\n",
    "    if Memo.get(person):\n",
    "        return Memo[person]\n",
    "\n",
    "    layer_type = hyperparameters[Encoding[0]][person[0]]\n",
    "    hidden_dim = [hyperparameters[Encoding[1]][person[1]], hyperparameters[Encoding[2]][person[2]], hyperparameters[Encoding[3]][person[3]], hyperparameters[Encoding[4]][person[4]]]\n",
    "    models = [GNNModelEA0, GNNModelEA1, GNNModelEA2, GNNModelEA3, GNNModelEA4]\n",
    "    for i in range(len(hidden_dim)):\n",
    "        if hidden_dim[i] == 0:\n",
    "            break\n",
    "    GNNmodel = models[i]\n",
    "    activation_func = hyperparameters[Encoding[5]][person[5]]\n",
    "    pooling = hyperparameters[Encoding[6]][person[6]]\n",
    "    lr = hyperparameters[Encoding[7]][person[7]]\n",
    "    batch_size = hyperparameters[Encoding[8]][person[8]]\n",
    "    weight_decay = hyperparameters[Encoding[9]][person[9]]\n",
    "\n",
    "    print(\"Layer Type:\", layer_type.__name__)\n",
    "    print(\"Hidden Dimensions:\", hidden_dim)\n",
    "    print(\"Activation Function:\", activation_func.__name__)\n",
    "    print(\"Pooling Function:\", pooling.__name__)\n",
    "    print(\"Learning Rate:\", lr)\n",
    "    print(\"Batch Size:\", batch_size)\n",
    "    print(\"Weight Decay:\", weight_decay)\n",
    "    \n",
    "    # Define the output path for saving the trained model\n",
    "    output_path = 'Saved/' + root_path + '/'\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    output_path += 'model.pt'\n",
    "\n",
    "\n",
    "    # Create training and testing datasets\n",
    "    train_dataset = CustomGraphDataset(root_path, split='train')\n",
    "    test_dataset = CustomGraphDataset(root_path, split='test')\n",
    "    # Define GNN model\n",
    "    input_dim = 7\n",
    "    output_dim = 10\n",
    "    model = GNNmodel(input_dim, output_dim, hidden_dim=hidden_dim, activation_func=activation_func, layer=layer_type, pooling=pooling).to(device)\n",
    "    # Define optimizer and loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Evaluate the model before training and print initial results\n",
    "    test_acc, test_loss = evaluate(model, test_loader, criterion, device)\n",
    "    print(f'Epoch {0}/{epochs} => 'f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}')\n",
    "\n",
    "    # Lists to store results for plotting\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    # Train the model and capture results\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_acc, train_loss = train(model, train_loader, optimizer, criterion, device)\n",
    "        test_acc, test_loss = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "        train_accuracies.append(train_acc)\n",
    "        train_losses.append(train_loss)\n",
    "        test_accuracies.append(test_acc)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch}/{epochs} => '\n",
    "                  f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f} | '\n",
    "                  f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}')\n",
    "\n",
    "    Memo[person] = test_accuracies[-1]\n",
    "    # Save the trained model\n",
    "    #model.save_model(output_path)\n",
    "    return test_accuracies[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gene_details(index, person):\n",
    "\n",
    "    layer_type = hyperparameters[Encoding[0]][person[0]]\n",
    "    hidden_dims = [hyperparameters[Encoding[1]][person[1]], hyperparameters[Encoding[2]][person[2]], hyperparameters[Encoding[3]][person[3]], hyperparameters[Encoding[4]][person[4]]]\n",
    "    activation_func = hyperparameters[Encoding[5]][person[5]]\n",
    "    pooling = hyperparameters[Encoding[6]][person[6]]\n",
    "    lr = hyperparameters[Encoding[7]][person[7]]\n",
    "    batch_size = hyperparameters[Encoding[8]][person[8]]\n",
    "    weight_decay = hyperparameters[Encoding[9]][person[9]]\n",
    "\n",
    "    print(f\"Gene {index} Details:\")\n",
    "    print(\"Layer Type:\", layer_type.__name__)\n",
    "    print(\"Hidden Dimensions:\", hidden_dims)\n",
    "    print(\"Activation Function:\", activation_func.__name__)\n",
    "    print(\"Pooling Function:\", pooling.__name__)\n",
    "    print(\"Learning Rate:\", lr)\n",
    "    print(\"Batch Size:\", batch_size)\n",
    "    print(\"Weight Decay:\", weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "persons = generate_population(10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer Type: GATConv\n",
      "Hidden Dimensions: [16, 64, 32, 16]\n",
      "Activation Function: tanh\n",
      "Pooling Function: global_max_pool\n",
      "Learning Rate: 0.1\n",
      "Batch Size: 8\n",
      "Weight Decay: 0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/50 => Test Loss: 2.3393, Test Accuracy: 0.1033\n",
      "Epoch 10/50 => Train Loss: 2.7301, Train Accuracy: 0.3083 | Test Loss: 3.5548, Test Accuracy: 0.2026\n",
      "Epoch 20/50 => Train Loss: 2.6762, Train Accuracy: 0.2796 | Test Loss: 2.6051, Test Accuracy: 0.2601\n",
      "Epoch 30/50 => Train Loss: 2.4078, Train Accuracy: 0.3815 | Test Loss: 3.0862, Test Accuracy: 0.3020\n",
      "Epoch 40/50 => Train Loss: 2.6963, Train Accuracy: 0.3171 | Test Loss: 2.4618, Test Accuracy: 0.2418\n",
      "Epoch 50/50 => Train Loss: 2.8278, Train Accuracy: 0.2698 | Test Loss: 2.9118, Test Accuracy: 0.2431\n",
      "Layer Type: GATConv\n",
      "Hidden Dimensions: [128, 0, 32, 64]\n",
      "Activation Function: relu\n",
      "Pooling Function: global_max_pool\n",
      "Learning Rate: 0.1\n",
      "Batch Size: 4\n",
      "Weight Decay: 1e-05\n",
      "Epoch 0/50 => Test Loss: 9.5364, Test Accuracy: 0.0641\n",
      "Epoch 10/50 => Train Loss: 23.4947, Train Accuracy: 0.2083 | Test Loss: 9.4023, Test Accuracy: 0.1268\n",
      "Epoch 20/50 => Train Loss: 2.2722, Train Accuracy: 0.1890 | Test Loss: 2.5229, Test Accuracy: 0.1124\n",
      "Epoch 30/50 => Train Loss: 2.1905, Train Accuracy: 0.1878 | Test Loss: 2.4310, Test Accuracy: 0.0954\n",
      "Epoch 40/50 => Train Loss: 2.1807, Train Accuracy: 0.1827 | Test Loss: 2.4535, Test Accuracy: 0.0954\n",
      "Epoch 50/50 => Train Loss: 2.3397, Train Accuracy: 0.1947 | Test Loss: 2.4907, Test Accuracy: 0.0954\n",
      "Layer Type: GATConv\n",
      "Hidden Dimensions: [8, 4, 4, 64]\n",
      "Activation Function: tanh\n",
      "Pooling Function: global_max_pool\n",
      "Learning Rate: 0.001\n",
      "Batch Size: 4\n",
      "Weight Decay: 1e-05\n",
      "Epoch 0/50 => Test Loss: 2.2934, Test Accuracy: 0.0980\n",
      "Epoch 10/50 => Train Loss: 1.5114, Train Accuracy: 0.4626 | Test Loss: 1.8110, Test Accuracy: 0.3268\n",
      "Epoch 20/50 => Train Loss: 1.3173, Train Accuracy: 0.5749 | Test Loss: 1.5737, Test Accuracy: 0.4444\n",
      "Epoch 30/50 => Train Loss: 1.0562, Train Accuracy: 0.6472 | Test Loss: 1.3469, Test Accuracy: 0.5176\n",
      "Epoch 40/50 => Train Loss: 0.9828, Train Accuracy: 0.6605 | Test Loss: 1.2688, Test Accuracy: 0.5804\n",
      "Epoch 50/50 => Train Loss: 0.9268, Train Accuracy: 0.7031 | Test Loss: 1.2374, Test Accuracy: 0.6131\n",
      "Layer Type: GATConv\n",
      "Hidden Dimensions: [0, 0, 64, 0]\n",
      "Activation Function: sigmoid\n",
      "Pooling Function: global_max_pool\n",
      "Learning Rate: 0.005\n",
      "Batch Size: 16\n",
      "Weight Decay: 0.001\n",
      "Epoch 0/50 => Test Loss: 2.3003, Test Accuracy: 0.0693\n",
      "Epoch 10/50 => Train Loss: 1.9512, Train Accuracy: 0.3642 | Test Loss: 2.0319, Test Accuracy: 0.2523\n",
      "Epoch 20/50 => Train Loss: 1.9318, Train Accuracy: 0.3736 | Test Loss: 2.0148, Test Accuracy: 0.2810\n",
      "Epoch 30/50 => Train Loss: 1.9154, Train Accuracy: 0.3739 | Test Loss: 1.9884, Test Accuracy: 0.2758\n",
      "Epoch 40/50 => Train Loss: 1.9145, Train Accuracy: 0.3736 | Test Loss: 1.9820, Test Accuracy: 0.2837\n",
      "Epoch 50/50 => Train Loss: 1.9099, Train Accuracy: 0.3774 | Test Loss: 1.9796, Test Accuracy: 0.2863\n",
      "Layer Type: GATConv\n",
      "Hidden Dimensions: [4, 0, 64, 64]\n",
      "Activation Function: sigmoid\n",
      "Pooling Function: global_max_pool\n",
      "Learning Rate: 0.001\n",
      "Batch Size: 4\n",
      "Weight Decay: 1e-05\n",
      "Epoch 0/50 => Test Loss: 2.3899, Test Accuracy: 0.1451\n",
      "Epoch 10/50 => Train Loss: 1.7165, Train Accuracy: 0.4235 | Test Loss: 1.9884, Test Accuracy: 0.2732\n",
      "Epoch 20/50 => Train Loss: 1.5585, Train Accuracy: 0.4819 | Test Loss: 1.8489, Test Accuracy: 0.3176\n",
      "Epoch 30/50 => Train Loss: 1.4719, Train Accuracy: 0.5219 | Test Loss: 1.7621, Test Accuracy: 0.3647\n",
      "Epoch 40/50 => Train Loss: 1.4344, Train Accuracy: 0.5323 | Test Loss: 1.7408, Test Accuracy: 0.3686\n",
      "Epoch 50/50 => Train Loss: 1.4077, Train Accuracy: 0.5342 | Test Loss: 1.7147, Test Accuracy: 0.3699\n",
      "Layer Type: GATConv\n",
      "Hidden Dimensions: [64, 8, 8, 8]\n",
      "Activation Function: relu\n",
      "Pooling Function: global_max_pool\n",
      "Learning Rate: 0.05\n",
      "Batch Size: 16\n",
      "Weight Decay: 0.0005\n",
      "Epoch 0/50 => Test Loss: 5.9970, Test Accuracy: 0.0758\n",
      "Epoch 10/50 => Train Loss: 2.1784, Train Accuracy: 0.2023 | Test Loss: 2.4174, Test Accuracy: 0.0941\n",
      "Epoch 20/50 => Train Loss: 2.1729, Train Accuracy: 0.2007 | Test Loss: 2.4336, Test Accuracy: 0.0941\n",
      "Epoch 30/50 => Train Loss: 2.1776, Train Accuracy: 0.1953 | Test Loss: 2.3607, Test Accuracy: 0.0941\n",
      "Epoch 40/50 => Train Loss: 2.1780, Train Accuracy: 0.1953 | Test Loss: 2.3977, Test Accuracy: 0.0941\n",
      "Epoch 50/50 => Train Loss: 2.1812, Train Accuracy: 0.1969 | Test Loss: 2.4045, Test Accuracy: 0.0941\n",
      "Layer Type: GCNConv\n",
      "Hidden Dimensions: [64, 8, 16, 4]\n",
      "Activation Function: tanh\n",
      "Pooling Function: global_mean_pool\n",
      "Learning Rate: 0.01\n",
      "Batch Size: 16\n",
      "Weight Decay: 0.001\n",
      "Epoch 0/50 => Test Loss: 2.3762, Test Accuracy: 0.0837\n",
      "Epoch 10/50 => Train Loss: 0.6525, Train Accuracy: 0.7772 | Test Loss: 0.8534, Test Accuracy: 0.6967\n",
      "Epoch 20/50 => Train Loss: 0.5532, Train Accuracy: 0.8135 | Test Loss: 0.7535, Test Accuracy: 0.7399\n",
      "Epoch 30/50 => Train Loss: 0.4852, Train Accuracy: 0.8400 | Test Loss: 0.6903, Test Accuracy: 0.7647\n",
      "Epoch 40/50 => Train Loss: 0.4361, Train Accuracy: 0.8498 | Test Loss: 0.6963, Test Accuracy: 0.7556\n",
      "Epoch 50/50 => Train Loss: 0.4201, Train Accuracy: 0.8621 | Test Loss: 0.6914, Test Accuracy: 0.7765\n",
      "Layer Type: GCNConv\n",
      "Hidden Dimensions: [8, 16, 0, 64]\n",
      "Activation Function: sigmoid\n",
      "Pooling Function: global_max_pool\n",
      "Learning Rate: 0.05\n",
      "Batch Size: 32\n",
      "Weight Decay: 0.001\n",
      "Epoch 0/50 => Test Loss: 2.5802, Test Accuracy: 0.1085\n",
      "Epoch 10/50 => Train Loss: 1.0436, Train Accuracy: 0.6381 | Test Loss: 1.1835, Test Accuracy: 0.6248\n",
      "Epoch 20/50 => Train Loss: 0.9249, Train Accuracy: 0.6898 | Test Loss: 1.0690, Test Accuracy: 0.6497\n",
      "Epoch 30/50 => Train Loss: 0.9325, Train Accuracy: 0.6971 | Test Loss: 1.2726, Test Accuracy: 0.5908\n",
      "Epoch 40/50 => Train Loss: 0.9205, Train Accuracy: 0.6920 | Test Loss: 1.1825, Test Accuracy: 0.6013\n",
      "Epoch 50/50 => Train Loss: 0.9131, Train Accuracy: 0.7062 | Test Loss: 1.0961, Test Accuracy: 0.6536\n",
      "Layer Type: GATConv\n",
      "Hidden Dimensions: [4, 4, 64, 16]\n",
      "Activation Function: relu\n",
      "Pooling Function: global_add_pool\n",
      "Learning Rate: 0.05\n",
      "Batch Size: 32\n",
      "Weight Decay: 1e-05\n",
      "Epoch 0/50 => Test Loss: 3134.8351, Test Accuracy: 0.1059\n",
      "Epoch 10/50 => Train Loss: 9.8074, Train Accuracy: 0.1243 | Test Loss: 11.9943, Test Accuracy: 0.0549\n",
      "Epoch 20/50 => Train Loss: 7.9923, Train Accuracy: 0.1363 | Test Loss: 20.2908, Test Accuracy: 0.1020\n",
      "Epoch 30/50 => Train Loss: 10.7461, Train Accuracy: 0.1426 | Test Loss: 10.8022, Test Accuracy: 0.0810\n",
      "Epoch 40/50 => Train Loss: 73.7698, Train Accuracy: 0.1483 | Test Loss: 56.6092, Test Accuracy: 0.1020\n",
      "Epoch 50/50 => Train Loss: 9.1410, Train Accuracy: 0.1464 | Test Loss: 12.1638, Test Accuracy: 0.1242\n",
      "Layer Type: GCNConv\n",
      "Hidden Dimensions: [64, 0, 0, 128]\n",
      "Activation Function: sigmoid\n",
      "Pooling Function: global_max_pool\n",
      "Learning Rate: 0.005\n",
      "Batch Size: 16\n",
      "Weight Decay: 0.001\n",
      "Epoch 0/50 => Test Loss: 2.8281, Test Accuracy: 0.0915\n",
      "Epoch 10/50 => Train Loss: 0.9437, Train Accuracy: 0.6977 | Test Loss: 1.2506, Test Accuracy: 0.5830\n",
      "Epoch 20/50 => Train Loss: 0.8066, Train Accuracy: 0.7315 | Test Loss: 1.1401, Test Accuracy: 0.6575\n",
      "Epoch 30/50 => Train Loss: 0.7405, Train Accuracy: 0.7498 | Test Loss: 1.0811, Test Accuracy: 0.6510\n",
      "Epoch 40/50 => Train Loss: 0.6930, Train Accuracy: 0.7681 | Test Loss: 1.1636, Test Accuracy: 0.6392\n",
      "Epoch 50/50 => Train Loss: 0.6441, Train Accuracy: 0.7835 | Test Loss: 1.1958, Test Accuracy: 0.6327\n",
      "Gen 0: \n",
      "Layer Type: GCNConv\n",
      "Hidden Dimensions: [16, 16, 32, 64]\n",
      "Activation Function: tanh\n",
      "Pooling Function: global_max_pool\n",
      "Learning Rate: 0.1\n",
      "Batch Size: 32\n",
      "Weight Decay: 0.001\n",
      "Epoch 0/50 => Test Loss: 2.3729, Test Accuracy: 0.0471\n",
      "Epoch 10/50 => Train Loss: 1.3557, Train Accuracy: 0.5648 | Test Loss: 1.7485, Test Accuracy: 0.4340\n",
      "Epoch 20/50 => Train Loss: 1.4075, Train Accuracy: 0.5585 | Test Loss: 1.9447, Test Accuracy: 0.4078\n",
      "Epoch 30/50 => Train Loss: 1.2931, Train Accuracy: 0.5535 | Test Loss: 1.7111, Test Accuracy: 0.4209\n",
      "Epoch 40/50 => Train Loss: 1.5393, Train Accuracy: 0.5308 | Test Loss: 1.6048, Test Accuracy: 0.4693\n",
      "Epoch 50/50 => Train Loss: 1.2713, Train Accuracy: 0.5731 | Test Loss: 1.3442, Test Accuracy: 0.5229\n",
      "Layer Type: GATConv\n",
      "Hidden Dimensions: [8, 64, 0, 16]\n",
      "Activation Function: sigmoid\n",
      "Pooling Function: global_max_pool\n",
      "Learning Rate: 0.05\n",
      "Batch Size: 8\n",
      "Weight Decay: 0.001\n",
      "Epoch 0/50 => Test Loss: 2.3960, Test Accuracy: 0.1111\n",
      "Epoch 10/50 => Train Loss: 1.2803, Train Accuracy: 0.5522 | Test Loss: 1.5940, Test Accuracy: 0.4209\n",
      "Epoch 20/50 => Train Loss: 1.1419, Train Accuracy: 0.5970 | Test Loss: 1.4156, Test Accuracy: 0.4824\n",
      "Epoch 30/50 => Train Loss: 1.3862, Train Accuracy: 0.5175 | Test Loss: 1.5504, Test Accuracy: 0.4536\n",
      "Epoch 40/50 => Train Loss: 1.4090, Train Accuracy: 0.5368 | Test Loss: 1.5948, Test Accuracy: 0.4458\n",
      "Epoch 50/50 => Train Loss: 1.1912, Train Accuracy: 0.5967 | Test Loss: 1.4963, Test Accuracy: 0.5059\n",
      "Layer Type: GCNConv\n",
      "Hidden Dimensions: [64, 4, 0, 16]\n",
      "Activation Function: sigmoid\n",
      "Pooling Function: global_add_pool\n",
      "Learning Rate: 0.005\n",
      "Batch Size: 32\n",
      "Weight Decay: 0.001\n",
      "Epoch 0/50 => Test Loss: 113.1915, Test Accuracy: 0.1203\n",
      "Epoch 10/50 => Train Loss: 2.3744, Train Accuracy: 0.4216 | Test Loss: 2.7234, Test Accuracy: 0.3634\n",
      "Epoch 20/50 => Train Loss: 1.9926, Train Accuracy: 0.5364 | Test Loss: 2.6245, Test Accuracy: 0.4510\n",
      "Epoch 30/50 => Train Loss: 1.9308, Train Accuracy: 0.5746 | Test Loss: 3.0842, Test Accuracy: 0.3987\n",
      "Epoch 40/50 => Train Loss: 1.6025, Train Accuracy: 0.5961 | Test Loss: 2.5534, Test Accuracy: 0.4837\n",
      "Epoch 50/50 => Train Loss: 2.0524, Train Accuracy: 0.5753 | Test Loss: 2.5675, Test Accuracy: 0.4444\n",
      "Layer Type: GCNConv\n",
      "Hidden Dimensions: [4, 0, 64, 128]\n",
      "Activation Function: relu\n",
      "Pooling Function: global_max_pool\n",
      "Learning Rate: 0.05\n",
      "Batch Size: 16\n",
      "Weight Decay: 1e-05\n",
      "Epoch 0/50 => Test Loss: 24.1009, Test Accuracy: 0.0980\n",
      "Epoch 10/50 => Train Loss: 2.0609, Train Accuracy: 0.2919 | Test Loss: 2.2546, Test Accuracy: 0.1791\n",
      "Epoch 20/50 => Train Loss: 1.8511, Train Accuracy: 0.3468 | Test Loss: 2.0697, Test Accuracy: 0.1974\n",
      "Epoch 30/50 => Train Loss: 2.2047, Train Accuracy: 0.2001 | Test Loss: 2.3697, Test Accuracy: 0.1137\n",
      "Epoch 40/50 => Train Loss: 2.0360, Train Accuracy: 0.2518 | Test Loss: 2.3765, Test Accuracy: 0.1464\n",
      "Epoch 50/50 => Train Loss: 2.0293, Train Accuracy: 0.2635 | Test Loss: 2.3262, Test Accuracy: 0.1503\n",
      "Layer Type: GCNConv\n",
      "Hidden Dimensions: [8, 16, 0, 64]\n",
      "Activation Function: sigmoid\n",
      "Pooling Function: global_max_pool\n",
      "Learning Rate: 0.05\n",
      "Batch Size: 32\n",
      "Weight Decay: 0.0005\n",
      "Epoch 0/50 => Test Loss: 2.8468, Test Accuracy: 0.0784\n",
      "Epoch 10/50 => Train Loss: 1.0219, Train Accuracy: 0.6620 | Test Loss: 1.2565, Test Accuracy: 0.5503\n",
      "Epoch 20/50 => Train Loss: 0.9433, Train Accuracy: 0.6949 | Test Loss: 1.1829, Test Accuracy: 0.6078\n",
      "Epoch 30/50 => Train Loss: 0.9317, Train Accuracy: 0.6863 | Test Loss: 1.1142, Test Accuracy: 0.6209\n",
      "Epoch 40/50 => Train Loss: 0.8638, Train Accuracy: 0.7021 | Test Loss: 1.0991, Test Accuracy: 0.6039\n",
      "Epoch 50/50 => Train Loss: 0.8926, Train Accuracy: 0.7040 | Test Loss: 1.0769, Test Accuracy: 0.6314\n",
      "Layer Type: GCNConv\n",
      "Hidden Dimensions: [64, 128, 0, 128]\n",
      "Activation Function: sigmoid\n",
      "Pooling Function: global_max_pool\n",
      "Learning Rate: 0.005\n",
      "Batch Size: 32\n",
      "Weight Decay: 0.001\n",
      "Epoch 0/50 => Test Loss: 2.6771, Test Accuracy: 0.1007\n",
      "Epoch 10/50 => Train Loss: 0.8545, Train Accuracy: 0.7160 | Test Loss: 1.1329, Test Accuracy: 0.5935\n",
      "Epoch 20/50 => Train Loss: 0.7090, Train Accuracy: 0.7567 | Test Loss: 0.9060, Test Accuracy: 0.6915\n",
      "Epoch 30/50 => Train Loss: 0.6281, Train Accuracy: 0.7867 | Test Loss: 0.8495, Test Accuracy: 0.7268\n",
      "Epoch 40/50 => Train Loss: 0.5503, Train Accuracy: 0.8151 | Test Loss: 0.8414, Test Accuracy: 0.7399\n",
      "Epoch 50/50 => Train Loss: 0.4986, Train Accuracy: 0.8312 | Test Loss: 0.9167, Test Accuracy: 0.7255\n",
      "Layer Type: GATConv\n",
      "Hidden Dimensions: [8, 0, 4, 0]\n",
      "Activation Function: tanh\n",
      "Pooling Function: global_max_pool\n",
      "Learning Rate: 0.001\n",
      "Batch Size: 16\n",
      "Weight Decay: 1e-05\n",
      "Epoch 0/50 => Test Loss: 2.3647, Test Accuracy: 0.0706\n",
      "Epoch 10/50 => Train Loss: 1.5806, Train Accuracy: 0.5229 | Test Loss: 1.8506, Test Accuracy: 0.3935\n",
      "Epoch 20/50 => Train Loss: 1.3686, Train Accuracy: 0.5939 | Test Loss: 1.6528, Test Accuracy: 0.4810\n",
      "Epoch 30/50 => Train Loss: 1.2403, Train Accuracy: 0.6191 | Test Loss: 1.5020, Test Accuracy: 0.5255\n",
      "Epoch 40/50 => Train Loss: 1.1658, Train Accuracy: 0.6387 | Test Loss: 1.4273, Test Accuracy: 0.5608\n",
      "Epoch 50/50 => Train Loss: 1.0982, Train Accuracy: 0.6611 | Test Loss: 1.3622, Test Accuracy: 0.5686\n",
      "Layer Type: GATConv\n",
      "Hidden Dimensions: [0, 4, 64, 64]\n",
      "Activation Function: sigmoid\n",
      "Pooling Function: global_max_pool\n",
      "Learning Rate: 0.005\n",
      "Batch Size: 4\n",
      "Weight Decay: 0.001\n",
      "Epoch 0/50 => Test Loss: 2.3226, Test Accuracy: 0.0745\n",
      "Epoch 10/50 => Train Loss: 1.9459, Train Accuracy: 0.2808 | Test Loss: 2.0221, Test Accuracy: 0.1739\n",
      "Epoch 20/50 => Train Loss: 1.9283, Train Accuracy: 0.2815 | Test Loss: 2.0022, Test Accuracy: 0.1895\n",
      "Epoch 30/50 => Train Loss: 1.9219, Train Accuracy: 0.2903 | Test Loss: 1.9931, Test Accuracy: 0.1882\n",
      "Epoch 40/50 => Train Loss: 1.9155, Train Accuracy: 0.2916 | Test Loss: 1.9842, Test Accuracy: 0.2105\n",
      "Epoch 50/50 => Train Loss: 1.9146, Train Accuracy: 0.2960 | Test Loss: 1.9844, Test Accuracy: 0.1974\n",
      "Layer Type: GATConv\n",
      "Hidden Dimensions: [64, 4, 0, 16]\n",
      "Activation Function: sigmoid\n",
      "Pooling Function: global_add_pool\n",
      "Learning Rate: 0.005\n",
      "Batch Size: 16\n",
      "Weight Decay: 0.001\n",
      "Epoch 0/50 => Test Loss: 155.3593, Test Accuracy: 0.1111\n",
      "Epoch 10/50 => Train Loss: 2.5117, Train Accuracy: 0.4156 | Test Loss: 3.6385, Test Accuracy: 0.3516\n",
      "Epoch 20/50 => Train Loss: 1.9119, Train Accuracy: 0.5942 | Test Loss: 2.6106, Test Accuracy: 0.5294\n",
      "Epoch 30/50 => Train Loss: 1.9909, Train Accuracy: 0.6280 | Test Loss: 3.1186, Test Accuracy: 0.5908\n",
      "Epoch 40/50 => Train Loss: 1.8042, Train Accuracy: 0.6519 | Test Loss: 2.4848, Test Accuracy: 0.6431\n",
      "Epoch 50/50 => Train Loss: 1.6472, Train Accuracy: 0.6747 | Test Loss: 2.6621, Test Accuracy: 0.5556\n",
      "Layer Type: GATConv\n",
      "Hidden Dimensions: [8, 4, 0, 64]\n",
      "Activation Function: tanh\n",
      "Pooling Function: global_max_pool\n",
      "Learning Rate: 0.001\n",
      "Batch Size: 4\n",
      "Weight Decay: 1e-05\n",
      "Epoch 0/50 => Test Loss: 2.3457, Test Accuracy: 0.0706\n",
      "Epoch 10/50 => Train Loss: 1.4745, Train Accuracy: 0.5216 | Test Loss: 1.7100, Test Accuracy: 0.3961\n",
      "Epoch 20/50 => Train Loss: 1.2486, Train Accuracy: 0.5737 | Test Loss: 1.4979, Test Accuracy: 0.4850\n",
      "Epoch 30/50 => Train Loss: 1.1622, Train Accuracy: 0.6056 | Test Loss: 1.4306, Test Accuracy: 0.4941\n",
      "Epoch 40/50 => Train Loss: 1.1006, Train Accuracy: 0.6207 | Test Loss: 1.3795, Test Accuracy: 0.5163\n",
      "Epoch 50/50 => Train Loss: 1.0551, Train Accuracy: 0.6191 | Test Loss: 1.3228, Test Accuracy: 0.5425\n",
      "Gen 1: \n",
      "Layer Type: GCNConv\n",
      "Hidden Dimensions: [8, 16, 128, 64]\n",
      "Activation Function: sigmoid\n",
      "Pooling Function: global_max_pool\n",
      "Learning Rate: 0.05\n",
      "Batch Size: 32\n",
      "Weight Decay: 0.0005\n",
      "Epoch 0/50 => Test Loss: 2.4977, Test Accuracy: 0.1111\n",
      "Epoch 10/50 => Train Loss: 1.9890, Train Accuracy: 0.3219 | Test Loss: 2.2501, Test Accuracy: 0.1804\n",
      "Epoch 20/50 => Train Loss: 1.5827, Train Accuracy: 0.5144 | Test Loss: 2.0305, Test Accuracy: 0.3386\n",
      "Epoch 30/50 => Train Loss: 1.0696, Train Accuracy: 0.6469 | Test Loss: 1.3659, Test Accuracy: 0.5516\n",
      "Epoch 40/50 => Train Loss: 0.9916, Train Accuracy: 0.6737 | Test Loss: 1.1388, Test Accuracy: 0.6196\n",
      "Epoch 50/50 => Train Loss: 0.9063, Train Accuracy: 0.6939 | Test Loss: 1.1530, Test Accuracy: 0.6196\n",
      "Layer Type: GCNConv\n",
      "Hidden Dimensions: [8, 8, 0, 4]\n",
      "Activation Function: sigmoid\n",
      "Pooling Function: global_mean_pool\n",
      "Learning Rate: 0.05\n",
      "Batch Size: 16\n",
      "Weight Decay: 0.001\n",
      "Epoch 0/50 => Test Loss: 2.3647, Test Accuracy: 0.0549\n",
      "Epoch 10/50 => Train Loss: 0.8723, Train Accuracy: 0.7059 | Test Loss: 0.9813, Test Accuracy: 0.6863\n",
      "Epoch 20/50 => Train Loss: 0.7642, Train Accuracy: 0.7460 | Test Loss: 0.9563, Test Accuracy: 0.6889\n",
      "Epoch 30/50 => Train Loss: 0.7471, Train Accuracy: 0.7479 | Test Loss: 0.9855, Test Accuracy: 0.6680\n",
      "Epoch 40/50 => Train Loss: 0.7354, Train Accuracy: 0.7586 | Test Loss: 0.9232, Test Accuracy: 0.6980\n",
      "Epoch 50/50 => Train Loss: 0.7296, Train Accuracy: 0.7539 | Test Loss: 0.8876, Test Accuracy: 0.6954\n",
      "Layer Type: GCNConv\n",
      "Hidden Dimensions: [64, 16, 8, 64]\n",
      "Activation Function: relu\n",
      "Pooling Function: global_max_pool\n",
      "Learning Rate: 0.01\n",
      "Batch Size: 32\n",
      "Weight Decay: 0.001\n",
      "Epoch 0/50 => Test Loss: 7.7603, Test Accuracy: 0.1059\n",
      "Epoch 10/50 => Train Loss: 2.1610, Train Accuracy: 0.2023 | Test Loss: 2.3942, Test Accuracy: 0.0941\n",
      "Epoch 20/50 => Train Loss: 2.1631, Train Accuracy: 0.2023 | Test Loss: 2.3952, Test Accuracy: 0.0941\n",
      "Epoch 30/50 => Train Loss: 2.1654, Train Accuracy: 0.2023 | Test Loss: 2.3904, Test Accuracy: 0.0941\n",
      "Epoch 40/50 => Train Loss: 2.1733, Train Accuracy: 0.2023 | Test Loss: 2.3963, Test Accuracy: 0.0941\n",
      "Epoch 50/50 => Train Loss: 2.1679, Train Accuracy: 0.2023 | Test Loss: 2.3925, Test Accuracy: 0.0941\n",
      "Layer Type: GCNConv\n",
      "Hidden Dimensions: [8, 16, 4, 64]\n",
      "Activation Function: tanh\n",
      "Pooling Function: global_max_pool\n",
      "Learning Rate: 0.001\n",
      "Batch Size: 32\n",
      "Weight Decay: 1e-05\n",
      "Epoch 0/50 => Test Loss: 2.3098, Test Accuracy: 0.1085\n",
      "Epoch 10/50 => Train Loss: 1.7413, Train Accuracy: 0.3840 | Test Loss: 2.0082, Test Accuracy: 0.2418\n",
      "Epoch 20/50 => Train Loss: 1.5403, Train Accuracy: 0.4408 | Test Loss: 1.8144, Test Accuracy: 0.2980\n",
      "Epoch 30/50 => Train Loss: 1.4249, Train Accuracy: 0.4617 | Test Loss: 1.6900, Test Accuracy: 0.3621\n",
      "Epoch 40/50 => Train Loss: 1.3337, Train Accuracy: 0.5349 | Test Loss: 1.5808, Test Accuracy: 0.4170\n",
      "Epoch 50/50 => Train Loss: 1.2499, Train Accuracy: 0.5778 | Test Loss: 1.4961, Test Accuracy: 0.4444\n",
      "Layer Type: GATConv\n",
      "Hidden Dimensions: [8, 4, 0, 64]\n",
      "Activation Function: sigmoid\n",
      "Pooling Function: global_max_pool\n",
      "Learning Rate: 0.05\n",
      "Batch Size: 4\n",
      "Weight Decay: 0.001\n",
      "Epoch 0/50 => Test Loss: 2.3106, Test Accuracy: 0.1216\n",
      "Epoch 10/50 => Train Loss: 1.2183, Train Accuracy: 0.5891 | Test Loss: 1.2865, Test Accuracy: 0.5163\n",
      "Epoch 20/50 => Train Loss: 1.1245, Train Accuracy: 0.6292 | Test Loss: 1.3247, Test Accuracy: 0.5464\n",
      "Epoch 30/50 => Train Loss: 1.0985, Train Accuracy: 0.6261 | Test Loss: 1.3762, Test Accuracy: 0.4889\n",
      "Epoch 40/50 => Train Loss: 1.1010, Train Accuracy: 0.6526 | Test Loss: 1.3406, Test Accuracy: 0.5333\n",
      "Epoch 50/50 => Train Loss: 1.1311, Train Accuracy: 0.6257 | Test Loss: 1.4558, Test Accuracy: 0.5098\n",
      "Layer Type: GCNConv\n",
      "Hidden Dimensions: [16, 0, 0, 128]\n",
      "Activation Function: tanh\n",
      "Pooling Function: global_max_pool\n",
      "Learning Rate: 0.05\n",
      "Batch Size: 16\n",
      "Weight Decay: 0.001\n",
      "Epoch 0/50 => Test Loss: 2.9402, Test Accuracy: 0.1190\n",
      "Epoch 10/50 => Train Loss: 1.0861, Train Accuracy: 0.6614 | Test Loss: 1.3758, Test Accuracy: 0.5856\n",
      "Epoch 20/50 => Train Loss: 0.9985, Train Accuracy: 0.6788 | Test Loss: 1.3034, Test Accuracy: 0.5908\n",
      "Epoch 30/50 => Train Loss: 1.0583, Train Accuracy: 0.6784 | Test Loss: 1.3646, Test Accuracy: 0.5817\n",
      "Epoch 40/50 => Train Loss: 1.0028, Train Accuracy: 0.6844 | Test Loss: 1.2207, Test Accuracy: 0.6144\n",
      "Epoch 50/50 => Train Loss: 0.9921, Train Accuracy: 0.6788 | Test Loss: 1.2738, Test Accuracy: 0.5817\n",
      "Layer Type: GCNConv\n",
      "Hidden Dimensions: [64, 16, 0, 64]\n",
      "Activation Function: sigmoid\n",
      "Pooling Function: global_max_pool\n",
      "Learning Rate: 0.005\n",
      "Batch Size: 32\n",
      "Weight Decay: 0.001\n",
      "Epoch 0/50 => Test Loss: 2.5291, Test Accuracy: 0.1229\n",
      "Epoch 10/50 => Train Loss: 1.0876, Train Accuracy: 0.6434 | Test Loss: 1.3426, Test Accuracy: 0.5503\n",
      "Epoch 20/50 => Train Loss: 0.8366, Train Accuracy: 0.7289 | Test Loss: 1.1218, Test Accuracy: 0.6418\n",
      "Epoch 30/50 => Train Loss: 0.7430, Train Accuracy: 0.7545 | Test Loss: 1.0495, Test Accuracy: 0.6771\n",
      "Epoch 40/50 => Train Loss: 0.6540, Train Accuracy: 0.7753 | Test Loss: 1.0314, Test Accuracy: 0.6706\n",
      "Epoch 50/50 => Train Loss: 0.6275, Train Accuracy: 0.7879 | Test Loss: 1.0013, Test Accuracy: 0.6758\n",
      "Layer Type: GCNConv\n",
      "Hidden Dimensions: [64, 128, 0, 128]\n",
      "Activation Function: relu\n",
      "Pooling Function: global_max_pool\n",
      "Learning Rate: 0.005\n",
      "Batch Size: 32\n",
      "Weight Decay: 0.001\n",
      "Epoch 0/50 => Test Loss: 5.8932, Test Accuracy: 0.1098\n",
      "Epoch 10/50 => Train Loss: 1.1953, Train Accuracy: 0.6504 | Test Loss: 1.4251, Test Accuracy: 0.5922\n",
      "Epoch 20/50 => Train Loss: 0.9285, Train Accuracy: 0.7144 | Test Loss: 1.2112, Test Accuracy: 0.6471\n",
      "Epoch 30/50 => Train Loss: 0.7283, Train Accuracy: 0.7602 | Test Loss: 1.0536, Test Accuracy: 0.7033\n",
      "Epoch 40/50 => Train Loss: 0.7097, Train Accuracy: 0.7810 | Test Loss: 1.0042, Test Accuracy: 0.7281\n",
      "Epoch 50/50 => Train Loss: 3.8634, Train Accuracy: 0.5535 | Test Loss: 2.0152, Test Accuracy: 0.5660\n",
      "Layer Type: GCNConv\n",
      "Hidden Dimensions: [8, 128, 0, 128]\n",
      "Activation Function: sigmoid\n",
      "Pooling Function: global_max_pool\n",
      "Learning Rate: 0.05\n",
      "Batch Size: 32\n",
      "Weight Decay: 0.0005\n",
      "Epoch 0/50 => Test Loss: 2.9914, Test Accuracy: 0.0954\n",
      "Epoch 10/50 => Train Loss: 1.1560, Train Accuracy: 0.6198 | Test Loss: 1.2921, Test Accuracy: 0.5281\n",
      "Epoch 20/50 => Train Loss: 1.1070, Train Accuracy: 0.6324 | Test Loss: 1.3030, Test Accuracy: 0.5660\n",
      "Epoch 30/50 => Train Loss: 0.9515, Train Accuracy: 0.6873 | Test Loss: 1.2494, Test Accuracy: 0.5699\n",
      "Epoch 40/50 => Train Loss: 0.8809, Train Accuracy: 0.7012 | Test Loss: 1.2330, Test Accuracy: 0.5765\n",
      "Epoch 50/50 => Train Loss: 0.9290, Train Accuracy: 0.6911 | Test Loss: 1.1400, Test Accuracy: 0.6314\n",
      "Layer Type: GATConv\n",
      "Hidden Dimensions: [8, 4, 0, 64]\n",
      "Activation Function: sigmoid\n",
      "Pooling Function: global_max_pool\n",
      "Learning Rate: 0.05\n",
      "Batch Size: 4\n",
      "Weight Decay: 0.0005\n",
      "Epoch 0/50 => Test Loss: 2.4042, Test Accuracy: 0.1020\n",
      "Epoch 10/50 => Train Loss: 1.4156, Train Accuracy: 0.5251 | Test Loss: 1.6152, Test Accuracy: 0.3974\n",
      "Epoch 20/50 => Train Loss: 1.3616, Train Accuracy: 0.5289 | Test Loss: 1.6680, Test Accuracy: 0.4000\n",
      "Epoch 30/50 => Train Loss: 1.2971, Train Accuracy: 0.5465 | Test Loss: 1.5461, Test Accuracy: 0.4588\n",
      "Epoch 40/50 => Train Loss: 1.3649, Train Accuracy: 0.5197 | Test Loss: 1.6304, Test Accuracy: 0.3948\n",
      "Epoch 50/50 => Train Loss: 1.3293, Train Accuracy: 0.5314 | Test Loss: 1.5236, Test Accuracy: 0.3948\n",
      "Gen 2: \n",
      "Layer Type: GCNConv\n",
      "Hidden Dimensions: [64, 16, 16, 64]\n",
      "Activation Function: tanh\n",
      "Pooling Function: global_max_pool\n",
      "Learning Rate: 0.01\n",
      "Batch Size: 32\n",
      "Weight Decay: 0.001\n",
      "Epoch 0/50 => Test Loss: 2.3356, Test Accuracy: 0.0915\n",
      "Epoch 10/50 => Train Loss: 0.8627, Train Accuracy: 0.7132 | Test Loss: 1.0244, Test Accuracy: 0.6667\n",
      "Epoch 20/50 => Train Loss: 0.7295, Train Accuracy: 0.7542 | Test Loss: 0.9344, Test Accuracy: 0.6837\n",
      "Epoch 30/50 => Train Loss: 0.6697, Train Accuracy: 0.7810 | Test Loss: 1.0131, Test Accuracy: 0.6876\n",
      "Epoch 40/50 => Train Loss: 0.7001, Train Accuracy: 0.7608 | Test Loss: 0.9350, Test Accuracy: 0.6850\n",
      "Epoch 50/50 => Train Loss: 0.5814, Train Accuracy: 0.8009 | Test Loss: 0.9312, Test Accuracy: 0.6954\n",
      "Layer Type: GCNConv\n",
      "Hidden Dimensions: [64, 128, 0, 128]\n",
      "Activation Function: sigmoid\n",
      "Pooling Function: global_max_pool\n",
      "Learning Rate: 0.01\n",
      "Batch Size: 32\n",
      "Weight Decay: 0.001\n",
      "Epoch 0/50 => Test Loss: 2.6506, Test Accuracy: 0.1229\n",
      "Epoch 10/50 => Train Loss: 0.8294, Train Accuracy: 0.7242 | Test Loss: 0.9782, Test Accuracy: 0.6928\n",
      "Epoch 20/50 => Train Loss: 0.6519, Train Accuracy: 0.7782 | Test Loss: 0.8841, Test Accuracy: 0.7386\n",
      "Epoch 30/50 => Train Loss: 0.5878, Train Accuracy: 0.8003 | Test Loss: 0.9022, Test Accuracy: 0.7281\n",
      "Epoch 40/50 => Train Loss: 0.5028, Train Accuracy: 0.8220 | Test Loss: 0.8632, Test Accuracy: 0.7464\n",
      "Epoch 50/50 => Train Loss: 0.4244, Train Accuracy: 0.8552 | Test Loss: 0.9686, Test Accuracy: 0.7242\n",
      "Layer Type: GCNConv\n",
      "Hidden Dimensions: [64, 128, 0, 128]\n",
      "Activation Function: sigmoid\n",
      "Pooling Function: global_mean_pool\n",
      "Learning Rate: 0.005\n",
      "Batch Size: 32\n",
      "Weight Decay: 0.001\n",
      "Epoch 0/50 => Test Loss: 2.3698, Test Accuracy: 0.1033\n",
      "Epoch 10/50 => Train Loss: 0.7838, Train Accuracy: 0.7422 | Test Loss: 0.9394, Test Accuracy: 0.6928\n",
      "Epoch 20/50 => Train Loss: 0.6419, Train Accuracy: 0.7936 | Test Loss: 0.7953, Test Accuracy: 0.7451\n",
      "Epoch 30/50 => Train Loss: 0.5892, Train Accuracy: 0.7999 | Test Loss: 0.7409, Test Accuracy: 0.7477\n",
      "Epoch 40/50 => Train Loss: 0.4917, Train Accuracy: 0.8321 | Test Loss: 0.6965, Test Accuracy: 0.7569\n",
      "Epoch 50/50 => Train Loss: 0.4316, Train Accuracy: 0.8602 | Test Loss: 0.7227, Test Accuracy: 0.7686\n",
      "Layer Type: GCNConv\n",
      "Hidden Dimensions: [64, 16, 0, 64]\n",
      "Activation Function: tanh\n",
      "Pooling Function: global_max_pool\n",
      "Learning Rate: 0.005\n",
      "Batch Size: 32\n",
      "Weight Decay: 0.001\n",
      "Epoch 0/50 => Test Loss: 2.4970, Test Accuracy: 0.0523\n",
      "Epoch 10/50 => Train Loss: 0.8095, Train Accuracy: 0.7365 | Test Loss: 1.0682, Test Accuracy: 0.6549\n",
      "Epoch 20/50 => Train Loss: 0.7018, Train Accuracy: 0.7643 | Test Loss: 1.0670, Test Accuracy: 0.6745\n",
      "Epoch 30/50 => Train Loss: 0.6290, Train Accuracy: 0.7914 | Test Loss: 0.9946, Test Accuracy: 0.6850\n",
      "Epoch 40/50 => Train Loss: 0.5674, Train Accuracy: 0.8104 | Test Loss: 0.9859, Test Accuracy: 0.6967\n",
      "Epoch 50/50 => Train Loss: 0.5543, Train Accuracy: 0.8211 | Test Loss: 0.9933, Test Accuracy: 0.7137\n",
      "Layer Type: GCNConv\n",
      "Hidden Dimensions: [64, 8, 0, 4]\n",
      "Activation Function: sigmoid\n",
      "Pooling Function: global_mean_pool\n",
      "Learning Rate: 0.005\n",
      "Batch Size: 16\n",
      "Weight Decay: 0.001\n",
      "Epoch 0/50 => Test Loss: 2.3957, Test Accuracy: 0.0980\n",
      "Epoch 10/50 => Train Loss: 0.9438, Train Accuracy: 0.6936 | Test Loss: 1.1150, Test Accuracy: 0.6248\n",
      "Epoch 20/50 => Train Loss: 0.7554, Train Accuracy: 0.7532 | Test Loss: 0.9662, Test Accuracy: 0.6706\n",
      "Epoch 30/50 => Train Loss: 0.6576, Train Accuracy: 0.7861 | Test Loss: 0.8270, Test Accuracy: 0.7359\n",
      "Epoch 40/50 => Train Loss: 0.5937, Train Accuracy: 0.8037 | Test Loss: 0.8003, Test Accuracy: 0.7255\n",
      "Epoch 50/50 => Train Loss: 0.5407, Train Accuracy: 0.8242 | Test Loss: 0.7745, Test Accuracy: 0.7320\n",
      "Gen 3: \n",
      "Layer Type: GCNConv\n",
      "Hidden Dimensions: [64, 128, 0, 128]\n",
      "Activation Function: sigmoid\n",
      "Pooling Function: global_max_pool\n",
      "Learning Rate: 0.005\n",
      "Batch Size: 32\n",
      "Weight Decay: 0.0005\n",
      "Epoch 0/50 => Test Loss: 2.8023, Test Accuracy: 0.1137\n",
      "Epoch 10/50 => Train Loss: 0.8435, Train Accuracy: 0.7188 | Test Loss: 1.1044, Test Accuracy: 0.6431\n",
      "Epoch 20/50 => Train Loss: 0.7414, Train Accuracy: 0.7608 | Test Loss: 1.0017, Test Accuracy: 0.6719\n",
      "Epoch 30/50 => Train Loss: 0.6406, Train Accuracy: 0.7778 | Test Loss: 0.9379, Test Accuracy: 0.7007\n",
      "Epoch 40/50 => Train Loss: 0.6209, Train Accuracy: 0.8031 | Test Loss: 0.8994, Test Accuracy: 0.6993\n",
      "Epoch 50/50 => Train Loss: 0.6798, Train Accuracy: 0.7914 | Test Loss: 0.8682, Test Accuracy: 0.7399\n",
      "Layer Type: GCNConv\n",
      "Hidden Dimensions: [64, 128, 8, 128]\n",
      "Activation Function: tanh\n",
      "Pooling Function: global_mean_pool\n",
      "Learning Rate: 0.01\n",
      "Batch Size: 32\n",
      "Weight Decay: 0.001\n",
      "Epoch 0/50 => Test Loss: 2.2743, Test Accuracy: 0.1007\n",
      "Epoch 10/50 => Train Loss: 0.6900, Train Accuracy: 0.7725 | Test Loss: 0.8448, Test Accuracy: 0.7072\n",
      "Epoch 20/50 => Train Loss: 0.5104, Train Accuracy: 0.8369 | Test Loss: 0.7785, Test Accuracy: 0.7399\n",
      "Epoch 30/50 => Train Loss: 0.4398, Train Accuracy: 0.8504 | Test Loss: 0.6928, Test Accuracy: 0.7673\n",
      "Epoch 40/50 => Train Loss: 0.3921, Train Accuracy: 0.8659 | Test Loss: 0.6985, Test Accuracy: 0.7817\n",
      "Epoch 50/50 => Train Loss: 0.3620, Train Accuracy: 0.8735 | Test Loss: 0.7139, Test Accuracy: 0.7712\n",
      "Layer Type: GATConv\n",
      "Hidden Dimensions: [64, 128, 0, 128]\n",
      "Activation Function: sigmoid\n",
      "Pooling Function: global_mean_pool\n",
      "Learning Rate: 0.005\n",
      "Batch Size: 32\n",
      "Weight Decay: 0.001\n",
      "Epoch 0/50 => Test Loss: 2.6767, Test Accuracy: 0.1085\n",
      "Epoch 10/50 => Train Loss: 0.6638, Train Accuracy: 0.7725 | Test Loss: 0.8142, Test Accuracy: 0.7098\n",
      "Epoch 20/50 => Train Loss: 0.5688, Train Accuracy: 0.8050 | Test Loss: 0.8658, Test Accuracy: 0.6824\n",
      "Epoch 30/50 => Train Loss: 0.4519, Train Accuracy: 0.8495 | Test Loss: 0.6795, Test Accuracy: 0.7634\n",
      "Epoch 40/50 => Train Loss: 0.4164, Train Accuracy: 0.8612 | Test Loss: 0.7425, Test Accuracy: 0.7373\n",
      "Epoch 50/50 => Train Loss: 0.3551, Train Accuracy: 0.8823 | Test Loss: 0.6473, Test Accuracy: 0.7882\n",
      "Layer Type: GCNConv\n",
      "Hidden Dimensions: [64, 0, 0, 128]\n",
      "Activation Function: sigmoid\n",
      "Pooling Function: global_max_pool\n",
      "Learning Rate: 0.005\n",
      "Batch Size: 32\n",
      "Weight Decay: 0.001\n",
      "Epoch 0/50 => Test Loss: 2.9480, Test Accuracy: 0.1072\n",
      "Epoch 10/50 => Train Loss: 0.9303, Train Accuracy: 0.7056 | Test Loss: 1.2121, Test Accuracy: 0.6026\n",
      "Epoch 20/50 => Train Loss: 0.8279, Train Accuracy: 0.7330 | Test Loss: 1.1376, Test Accuracy: 0.6510\n",
      "Epoch 30/50 => Train Loss: 0.7544, Train Accuracy: 0.7551 | Test Loss: 1.0654, Test Accuracy: 0.6510\n",
      "Epoch 40/50 => Train Loss: 0.7055, Train Accuracy: 0.7640 | Test Loss: 1.0482, Test Accuracy: 0.6601\n",
      "Epoch 50/50 => Train Loss: 0.6713, Train Accuracy: 0.7760 | Test Loss: 1.0677, Test Accuracy: 0.6497\n",
      "Layer Type: GCNConv\n",
      "Hidden Dimensions: [64, 8, 0, 4]\n",
      "Activation Function: sigmoid\n",
      "Pooling Function: global_mean_pool\n",
      "Learning Rate: 0.005\n",
      "Batch Size: 16\n",
      "Weight Decay: 1e-05\n",
      "Epoch 0/50 => Test Loss: 2.4499, Test Accuracy: 0.0549\n",
      "Epoch 10/50 => Train Loss: 0.9585, Train Accuracy: 0.6895 | Test Loss: 1.1549, Test Accuracy: 0.5830\n",
      "Epoch 20/50 => Train Loss: 0.7378, Train Accuracy: 0.7681 | Test Loss: 0.9253, Test Accuracy: 0.6954\n",
      "Epoch 30/50 => Train Loss: 0.6498, Train Accuracy: 0.7974 | Test Loss: 0.8118, Test Accuracy: 0.7346\n",
      "Epoch 40/50 => Train Loss: 0.5959, Train Accuracy: 0.8050 | Test Loss: 0.7722, Test Accuracy: 0.7386\n",
      "Epoch 50/50 => Train Loss: 0.5580, Train Accuracy: 0.8217 | Test Loss: 0.7422, Test Accuracy: 0.7425\n",
      "Gen 4: \n",
      "Layer Type: GCNConv\n",
      "Hidden Dimensions: [64, 128, 0, 128]\n",
      "Activation Function: sigmoid\n",
      "Pooling Function: global_mean_pool\n",
      "Learning Rate: 0.005\n",
      "Batch Size: 32\n",
      "Weight Decay: 0.0005\n",
      "Epoch 0/50 => Test Loss: 2.7010, Test Accuracy: 0.1020\n",
      "Epoch 10/50 => Train Loss: 0.7630, Train Accuracy: 0.7494 | Test Loss: 0.9482, Test Accuracy: 0.6993\n",
      "Epoch 20/50 => Train Loss: 0.6264, Train Accuracy: 0.7946 | Test Loss: 0.8122, Test Accuracy: 0.7412\n",
      "Epoch 30/50 => Train Loss: 0.5559, Train Accuracy: 0.8160 | Test Loss: 0.7846, Test Accuracy: 0.7216\n",
      "Epoch 40/50 => Train Loss: 0.5006, Train Accuracy: 0.8391 | Test Loss: 0.7398, Test Accuracy: 0.7556\n",
      "Epoch 50/50 => Train Loss: 0.4290, Train Accuracy: 0.8552 | Test Loss: 0.6256, Test Accuracy: 0.7882\n",
      "Layer Type: GCNConv\n",
      "Hidden Dimensions: [64, 128, 0, 64]\n",
      "Activation Function: sigmoid\n",
      "Pooling Function: global_max_pool\n",
      "Learning Rate: 0.005\n",
      "Batch Size: 32\n",
      "Weight Decay: 0.001\n",
      "Epoch 0/50 => Test Loss: 2.5036, Test Accuracy: 0.1007\n",
      "Epoch 10/50 => Train Loss: 0.8887, Train Accuracy: 0.7100 | Test Loss: 1.2555, Test Accuracy: 0.6078\n",
      "Epoch 20/50 => Train Loss: 0.7223, Train Accuracy: 0.7583 | Test Loss: 1.0316, Test Accuracy: 0.6732\n",
      "Epoch 30/50 => Train Loss: 0.6361, Train Accuracy: 0.7917 | Test Loss: 0.9914, Test Accuracy: 0.7033\n",
      "Epoch 40/50 => Train Loss: 0.5731, Train Accuracy: 0.8148 | Test Loss: 0.9531, Test Accuracy: 0.7020\n",
      "Epoch 50/50 => Train Loss: 0.5408, Train Accuracy: 0.8189 | Test Loss: 0.9209, Test Accuracy: 0.7346\n",
      "Layer Type: GATConv\n",
      "Hidden Dimensions: [64, 128, 0, 128]\n",
      "Activation Function: sigmoid\n",
      "Pooling Function: global_mean_pool\n",
      "Learning Rate: 0.005\n",
      "Batch Size: 32\n",
      "Weight Decay: 0.0005\n",
      "Epoch 0/50 => Test Loss: 2.5662, Test Accuracy: 0.0810\n",
      "Epoch 10/50 => Train Loss: 0.6831, Train Accuracy: 0.7586 | Test Loss: 0.8569, Test Accuracy: 0.6967\n",
      "Epoch 20/50 => Train Loss: 0.5414, Train Accuracy: 0.8141 | Test Loss: 0.7058, Test Accuracy: 0.7582\n",
      "Epoch 30/50 => Train Loss: 0.4726, Train Accuracy: 0.8419 | Test Loss: 0.6938, Test Accuracy: 0.7647\n",
      "Epoch 40/50 => Train Loss: 0.3921, Train Accuracy: 0.8646 | Test Loss: 0.6250, Test Accuracy: 0.8026\n",
      "Epoch 50/50 => Train Loss: 0.4031, Train Accuracy: 0.8653 | Test Loss: 0.6781, Test Accuracy: 0.7752\n",
      "Layer Type: GCNConv\n",
      "Hidden Dimensions: [32, 128, 8, 0]\n",
      "Activation Function: tanh\n",
      "Pooling Function: global_mean_pool\n",
      "Learning Rate: 0.01\n",
      "Batch Size: 32\n",
      "Weight Decay: 0.001\n",
      "Epoch 0/50 => Test Loss: 2.3241, Test Accuracy: 0.1386\n",
      "Epoch 10/50 => Train Loss: 0.6899, Train Accuracy: 0.7690 | Test Loss: 0.9118, Test Accuracy: 0.6941\n",
      "Epoch 20/50 => Train Loss: 0.5318, Train Accuracy: 0.8249 | Test Loss: 0.7490, Test Accuracy: 0.7621\n",
      "Epoch 30/50 => Train Loss: 0.4709, Train Accuracy: 0.8495 | Test Loss: 0.7927, Test Accuracy: 0.7451\n",
      "Epoch 40/50 => Train Loss: 0.4234, Train Accuracy: 0.8571 | Test Loss: 0.7237, Test Accuracy: 0.7725\n",
      "Epoch 50/50 => Train Loss: 0.4156, Train Accuracy: 0.8612 | Test Loss: 0.7334, Test Accuracy: 0.7660\n",
      "Layer Type: GCNConv\n",
      "Hidden Dimensions: [64, 128, 16, 128]\n",
      "Activation Function: tanh\n",
      "Pooling Function: global_mean_pool\n",
      "Learning Rate: 0.01\n",
      "Batch Size: 32\n",
      "Weight Decay: 0.001\n",
      "Epoch 0/50 => Test Loss: 2.3624, Test Accuracy: 0.0405\n",
      "Epoch 10/50 => Train Loss: 0.6173, Train Accuracy: 0.8085 | Test Loss: 0.7728, Test Accuracy: 0.7373\n",
      "Epoch 20/50 => Train Loss: 0.5648, Train Accuracy: 0.8081 | Test Loss: 0.7231, Test Accuracy: 0.7621\n",
      "Epoch 30/50 => Train Loss: 0.4206, Train Accuracy: 0.8593 | Test Loss: 0.6813, Test Accuracy: 0.7817\n",
      "Epoch 40/50 => Train Loss: 0.3721, Train Accuracy: 0.8725 | Test Loss: 0.7638, Test Accuracy: 0.7817\n",
      "Epoch 50/50 => Train Loss: 0.3400, Train Accuracy: 0.8873 | Test Loss: 0.7074, Test Accuracy: 0.7908\n",
      "Gen 5: \n",
      "Layer Type: GATConv\n",
      "Hidden Dimensions: [64, 128, 16, 128]\n",
      "Activation Function: tanh\n",
      "Pooling Function: global_mean_pool\n",
      "Learning Rate: 0.01\n",
      "Batch Size: 32\n",
      "Weight Decay: 0.001\n",
      "Epoch 0/50 => Test Loss: 2.3295, Test Accuracy: 0.1582\n",
      "Epoch 10/50 => Train Loss: 0.7130, Train Accuracy: 0.7624 | Test Loss: 0.8720, Test Accuracy: 0.6889\n",
      "Epoch 20/50 => Train Loss: 0.6112, Train Accuracy: 0.7902 | Test Loss: 0.7256, Test Accuracy: 0.7542\n",
      "Epoch 30/50 => Train Loss: 0.5608, Train Accuracy: 0.8085 | Test Loss: 0.7852, Test Accuracy: 0.7307\n",
      "Epoch 40/50 => Train Loss: 0.7719, Train Accuracy: 0.7375 | Test Loss: 1.0267, Test Accuracy: 0.6248\n",
      "Epoch 50/50 => Train Loss: 0.5211, Train Accuracy: 0.8173 | Test Loss: 0.7297, Test Accuracy: 0.7438\n",
      "Layer Type: GCNConv\n",
      "Hidden Dimensions: [64, 8, 0, 4]\n",
      "Activation Function: sigmoid\n",
      "Pooling Function: global_mean_pool\n",
      "Learning Rate: 0.005\n",
      "Batch Size: 16\n",
      "Weight Decay: 0.0005\n",
      "Epoch 0/50 => Test Loss: 2.4352, Test Accuracy: 0.1033\n",
      "Epoch 10/50 => Train Loss: 0.9228, Train Accuracy: 0.7005 | Test Loss: 1.0920, Test Accuracy: 0.6458\n",
      "Epoch 20/50 => Train Loss: 0.7443, Train Accuracy: 0.7649 | Test Loss: 0.9032, Test Accuracy: 0.7124\n",
      "Epoch 30/50 => Train Loss: 0.6381, Train Accuracy: 0.7968 | Test Loss: 0.8077, Test Accuracy: 0.7399\n",
      "Epoch 40/50 => Train Loss: 0.5839, Train Accuracy: 0.8078 | Test Loss: 0.7795, Test Accuracy: 0.7242\n",
      "Epoch 50/50 => Train Loss: 0.5491, Train Accuracy: 0.8223 | Test Loss: 0.7362, Test Accuracy: 0.7477\n",
      "Layer Type: GCNConv\n",
      "Hidden Dimensions: [128, 8, 16, 4]\n",
      "Activation Function: tanh\n",
      "Pooling Function: global_mean_pool\n",
      "Learning Rate: 0.01\n",
      "Batch Size: 16\n",
      "Weight Decay: 0.001\n",
      "Epoch 0/50 => Test Loss: 2.3083, Test Accuracy: 0.0889\n",
      "Epoch 10/50 => Train Loss: 0.6444, Train Accuracy: 0.7886 | Test Loss: 0.8924, Test Accuracy: 0.7020\n",
      "Epoch 20/50 => Train Loss: 0.5413, Train Accuracy: 0.8141 | Test Loss: 0.7967, Test Accuracy: 0.7425\n",
      "Epoch 30/50 => Train Loss: 0.4900, Train Accuracy: 0.8435 | Test Loss: 0.7723, Test Accuracy: 0.7503\n",
      "Epoch 40/50 => Train Loss: 0.4517, Train Accuracy: 0.8470 | Test Loss: 0.7971, Test Accuracy: 0.7438\n",
      "Epoch 50/50 => Train Loss: 0.4351, Train Accuracy: 0.8504 | Test Loss: 0.7610, Test Accuracy: 0.7556\n",
      "Layer Type: GATConv\n",
      "Hidden Dimensions: [64, 128, 0, 128]\n",
      "Activation Function: relu\n",
      "Pooling Function: global_mean_pool\n",
      "Learning Rate: 0.005\n",
      "Batch Size: 32\n",
      "Weight Decay: 0.001\n",
      "Epoch 0/50 => Test Loss: 5.5056, Test Accuracy: 0.0928\n",
      "Epoch 10/50 => Train Loss: 0.7273, Train Accuracy: 0.7820 | Test Loss: 0.8722, Test Accuracy: 0.7412\n",
      "Epoch 20/50 => Train Loss: 3.8265, Train Accuracy: 0.6257 | Test Loss: 2.1206, Test Accuracy: 0.6745\n",
      "Epoch 30/50 => Train Loss: 1.0939, Train Accuracy: 0.7551 | Test Loss: 1.3290, Test Accuracy: 0.6824\n",
      "Epoch 40/50 => Train Loss: 0.6195, Train Accuracy: 0.8104 | Test Loss: 1.3361, Test Accuracy: 0.7255\n",
      "Epoch 50/50 => Train Loss: 6.9616, Train Accuracy: 0.6478 | Test Loss: 4.4044, Test Accuracy: 0.6170\n",
      "Layer Type: GCNConv\n",
      "Hidden Dimensions: [64, 8, 16, 4]\n",
      "Activation Function: relu\n",
      "Pooling Function: global_mean_pool\n",
      "Learning Rate: 0.01\n",
      "Batch Size: 16\n",
      "Weight Decay: 0.001\n",
      "Epoch 0/50 => Test Loss: 4.5349, Test Accuracy: 0.1085\n",
      "Epoch 10/50 => Train Loss: 0.9831, Train Accuracy: 0.6693 | Test Loss: 1.2188, Test Accuracy: 0.5556\n",
      "Epoch 20/50 => Train Loss: 0.8325, Train Accuracy: 0.7296 | Test Loss: 1.0568, Test Accuracy: 0.6484\n",
      "Epoch 30/50 => Train Loss: 0.8420, Train Accuracy: 0.7043 | Test Loss: 1.1731, Test Accuracy: 0.6405\n",
      "Epoch 40/50 => Train Loss: 0.7183, Train Accuracy: 0.7756 | Test Loss: 1.0585, Test Accuracy: 0.6627\n",
      "Epoch 50/50 => Train Loss: 0.7676, Train Accuracy: 0.7570 | Test Loss: 0.9445, Test Accuracy: 0.6758\n",
      "Gen 6: \n",
      "Layer Type: GATConv\n",
      "Hidden Dimensions: [64, 128, 0, 128]\n",
      "Activation Function: sigmoid\n",
      "Pooling Function: global_max_pool\n",
      "Learning Rate: 0.005\n",
      "Batch Size: 32\n",
      "Weight Decay: 0.0005\n",
      "Epoch 0/50 => Test Loss: 2.7405, Test Accuracy: 0.1242\n",
      "Epoch 10/50 => Train Loss: 0.6431, Train Accuracy: 0.7737 | Test Loss: 0.8699, Test Accuracy: 0.6667\n",
      "Epoch 20/50 => Train Loss: 0.5765, Train Accuracy: 0.7927 | Test Loss: 0.7640, Test Accuracy: 0.7320\n",
      "Epoch 30/50 => Train Loss: 0.4773, Train Accuracy: 0.8350 | Test Loss: 0.9337, Test Accuracy: 0.6928\n",
      "Epoch 40/50 => Train Loss: 0.3965, Train Accuracy: 0.8593 | Test Loss: 0.8314, Test Accuracy: 0.7425\n",
      "Epoch 50/50 => Train Loss: 0.3913, Train Accuracy: 0.8640 | Test Loss: 0.7925, Test Accuracy: 0.7621\n",
      "Layer Type: GCNConv\n",
      "Hidden Dimensions: [64, 128, 16, 128]\n",
      "Activation Function: tanh\n",
      "Pooling Function: global_mean_pool\n",
      "Learning Rate: 0.05\n",
      "Batch Size: 32\n",
      "Weight Decay: 0.001\n",
      "Epoch 0/50 => Test Loss: 2.3540, Test Accuracy: 0.0209\n",
      "Epoch 10/50 => Train Loss: 0.8281, Train Accuracy: 0.7223 | Test Loss: 1.1650, Test Accuracy: 0.6026\n",
      "Epoch 20/50 => Train Loss: 0.7983, Train Accuracy: 0.7305 | Test Loss: 0.9784, Test Accuracy: 0.6693\n",
      "Epoch 30/50 => Train Loss: 0.8712, Train Accuracy: 0.7289 | Test Loss: 1.0196, Test Accuracy: 0.6758\n",
      "Epoch 40/50 => Train Loss: 0.7340, Train Accuracy: 0.7583 | Test Loss: 0.9176, Test Accuracy: 0.6941\n",
      "Epoch 50/50 => Train Loss: 0.7878, Train Accuracy: 0.7469 | Test Loss: 0.9694, Test Accuracy: 0.6719\n",
      "Layer Type: GCNConv\n",
      "Hidden Dimensions: [32, 128, 16, 128]\n",
      "Activation Function: tanh\n",
      "Pooling Function: global_max_pool\n",
      "Learning Rate: 0.01\n",
      "Batch Size: 32\n",
      "Weight Decay: 0.001\n",
      "Epoch 0/50 => Test Loss: 2.4179, Test Accuracy: 0.0980\n",
      "Epoch 10/50 => Train Loss: 0.8984, Train Accuracy: 0.6993 | Test Loss: 1.0532, Test Accuracy: 0.6196\n",
      "Epoch 20/50 => Train Loss: 0.7878, Train Accuracy: 0.7441 | Test Loss: 0.9350, Test Accuracy: 0.6993\n",
      "Epoch 30/50 => Train Loss: 0.6716, Train Accuracy: 0.7753 | Test Loss: 0.8875, Test Accuracy: 0.7163\n",
      "Epoch 40/50 => Train Loss: 0.6842, Train Accuracy: 0.7778 | Test Loss: 0.9120, Test Accuracy: 0.7020\n",
      "Epoch 50/50 => Train Loss: 0.7039, Train Accuracy: 0.7845 | Test Loss: 0.8970, Test Accuracy: 0.7007\n",
      "Layer Type: GCNConv\n",
      "Hidden Dimensions: [64, 128, 16, 64]\n",
      "Activation Function: tanh\n",
      "Pooling Function: global_mean_pool\n",
      "Learning Rate: 0.01\n",
      "Batch Size: 32\n",
      "Weight Decay: 0.001\n",
      "Epoch 0/50 => Test Loss: 2.3484, Test Accuracy: 0.0732\n",
      "Epoch 10/50 => Train Loss: 0.6523, Train Accuracy: 0.7920 | Test Loss: 0.8236, Test Accuracy: 0.7163\n",
      "Epoch 20/50 => Train Loss: 0.5008, Train Accuracy: 0.8331 | Test Loss: 0.6967, Test Accuracy: 0.7791\n",
      "Epoch 30/50 => Train Loss: 0.3713, Train Accuracy: 0.8782 | Test Loss: 0.6429, Test Accuracy: 0.7961\n",
      "Epoch 40/50 => Train Loss: 0.3829, Train Accuracy: 0.8697 | Test Loss: 0.6995, Test Accuracy: 0.7830\n",
      "Epoch 50/50 => Train Loss: 0.3158, Train Accuracy: 0.8933 | Test Loss: 0.6574, Test Accuracy: 0.8039\n",
      "Layer Type: GCNConv\n",
      "Hidden Dimensions: [64, 128, 128, 128]\n",
      "Activation Function: sigmoid\n",
      "Pooling Function: global_mean_pool\n",
      "Learning Rate: 0.005\n",
      "Batch Size: 32\n",
      "Weight Decay: 0.0005\n",
      "Epoch 0/50 => Test Loss: 2.4373, Test Accuracy: 0.0549\n",
      "Epoch 10/50 => Train Loss: 0.8072, Train Accuracy: 0.7242 | Test Loss: 0.9570, Test Accuracy: 0.6797\n",
      "Epoch 20/50 => Train Loss: 0.6863, Train Accuracy: 0.7775 | Test Loss: 0.8947, Test Accuracy: 0.7072\n",
      "Epoch 30/50 => Train Loss: 0.5597, Train Accuracy: 0.8056 | Test Loss: 0.7519, Test Accuracy: 0.7556\n",
      "Epoch 40/50 => Train Loss: 0.4575, Train Accuracy: 0.8482 | Test Loss: 0.6414, Test Accuracy: 0.7869\n",
      "Epoch 50/50 => Train Loss: 0.4624, Train Accuracy: 0.8375 | Test Loss: 0.6328, Test Accuracy: 0.7804\n",
      "Gen 7: \n",
      "Layer Type: GCNConv\n",
      "Hidden Dimensions: [64, 128, 16, 128]\n",
      "Activation Function: tanh\n",
      "Pooling Function: global_mean_pool\n",
      "Learning Rate: 0.005\n",
      "Batch Size: 32\n",
      "Weight Decay: 0.001\n",
      "Epoch 0/50 => Test Loss: 2.3307, Test Accuracy: 0.0850\n",
      "Epoch 10/50 => Train Loss: 0.5876, Train Accuracy: 0.7958 | Test Loss: 0.7647, Test Accuracy: 0.7477\n",
      "Epoch 20/50 => Train Loss: 0.4967, Train Accuracy: 0.8280 | Test Loss: 0.6708, Test Accuracy: 0.7608\n",
      "Epoch 30/50 => Train Loss: 0.3562, Train Accuracy: 0.8842 | Test Loss: 0.6781, Test Accuracy: 0.7791\n",
      "Epoch 40/50 => Train Loss: 0.3123, Train Accuracy: 0.8949 | Test Loss: 0.7174, Test Accuracy: 0.7608\n",
      "Epoch 50/50 => Train Loss: 0.2544, Train Accuracy: 0.9154 | Test Loss: 0.6830, Test Accuracy: 0.7948\n",
      "Layer Type: GCNConv\n",
      "Hidden Dimensions: [32, 128, 16, 128]\n",
      "Activation Function: tanh\n",
      "Pooling Function: global_mean_pool\n",
      "Learning Rate: 0.05\n",
      "Batch Size: 32\n",
      "Weight Decay: 0.001\n",
      "Epoch 0/50 => Test Loss: 2.3095, Test Accuracy: 0.0784\n",
      "Epoch 10/50 => Train Loss: 0.7860, Train Accuracy: 0.7419 | Test Loss: 0.9512, Test Accuracy: 0.6641\n",
      "Epoch 20/50 => Train Loss: 0.8270, Train Accuracy: 0.7255 | Test Loss: 0.8944, Test Accuracy: 0.7229\n",
      "Epoch 30/50 => Train Loss: 0.7313, Train Accuracy: 0.7542 | Test Loss: 0.9019, Test Accuracy: 0.7046\n",
      "Epoch 40/50 => Train Loss: 0.7454, Train Accuracy: 0.7526 | Test Loss: 0.9800, Test Accuracy: 0.6641\n",
      "Epoch 50/50 => Train Loss: 0.7466, Train Accuracy: 0.7558 | Test Loss: 0.9003, Test Accuracy: 0.6915\n",
      "Layer Type: GCNConv\n",
      "Hidden Dimensions: [64, 64, 16, 128]\n",
      "Activation Function: tanh\n",
      "Pooling Function: global_mean_pool\n",
      "Learning Rate: 0.01\n",
      "Batch Size: 32\n",
      "Weight Decay: 0.001\n",
      "Epoch 0/50 => Test Loss: 2.3226, Test Accuracy: 0.1150\n",
      "Epoch 10/50 => Train Loss: 0.5564, Train Accuracy: 0.8122 | Test Loss: 0.8110, Test Accuracy: 0.7216\n",
      "Epoch 20/50 => Train Loss: 0.4937, Train Accuracy: 0.8419 | Test Loss: 0.7192, Test Accuracy: 0.7608\n",
      "Epoch 30/50 => Train Loss: 0.4567, Train Accuracy: 0.8454 | Test Loss: 0.6577, Test Accuracy: 0.7830\n",
      "Epoch 40/50 => Train Loss: 0.3484, Train Accuracy: 0.8836 | Test Loss: 0.6674, Test Accuracy: 0.8026\n",
      "Epoch 50/50 => Train Loss: 0.3454, Train Accuracy: 0.8820 | Test Loss: 0.6871, Test Accuracy: 0.8039\n",
      "Gen 8: \n",
      "Layer Type: GCNConv\n",
      "Hidden Dimensions: [64, 64, 16, 128]\n",
      "Activation Function: tanh\n",
      "Pooling Function: global_add_pool\n",
      "Learning Rate: 0.01\n",
      "Batch Size: 32\n",
      "Weight Decay: 0.001\n",
      "Epoch 0/50 => Test Loss: 36.7933, Test Accuracy: 0.1216\n",
      "Epoch 10/50 => Train Loss: 6.3098, Train Accuracy: 0.4610 | Test Loss: 6.9059, Test Accuracy: 0.4784\n",
      "Epoch 20/50 => Train Loss: 7.7614, Train Accuracy: 0.5576 | Test Loss: 10.2342, Test Accuracy: 0.5111\n",
      "Epoch 30/50 => Train Loss: 5.7025, Train Accuracy: 0.6040 | Test Loss: 10.8386, Test Accuracy: 0.4680\n",
      "Epoch 40/50 => Train Loss: 6.4866, Train Accuracy: 0.6037 | Test Loss: 21.1735, Test Accuracy: 0.5163\n",
      "Epoch 50/50 => Train Loss: 6.8980, Train Accuracy: 0.6005 | Test Loss: 11.6505, Test Accuracy: 0.5412\n",
      "Layer Type: GCNConv\n",
      "Hidden Dimensions: [64, 128, 16, 64]\n",
      "Activation Function: tanh\n",
      "Pooling Function: global_mean_pool\n",
      "Learning Rate: 0.005\n",
      "Batch Size: 32\n",
      "Weight Decay: 0.001\n",
      "Epoch 0/50 => Test Loss: 2.3247, Test Accuracy: 0.0967\n",
      "Epoch 10/50 => Train Loss: 0.6309, Train Accuracy: 0.7902 | Test Loss: 0.8333, Test Accuracy: 0.7072\n",
      "Epoch 20/50 => Train Loss: 0.4714, Train Accuracy: 0.8470 | Test Loss: 0.6767, Test Accuracy: 0.7804\n",
      "Epoch 30/50 => Train Loss: 0.3970, Train Accuracy: 0.8709 | Test Loss: 0.5787, Test Accuracy: 0.8209\n",
      "Epoch 40/50 => Train Loss: 0.3402, Train Accuracy: 0.8930 | Test Loss: 0.6710, Test Accuracy: 0.8013\n",
      "Epoch 50/50 => Train Loss: 0.2755, Train Accuracy: 0.9110 | Test Loss: 0.6014, Test Accuracy: 0.8144\n",
      "Layer Type: GCNConv\n",
      "Hidden Dimensions: [64, 128, 16, 64]\n",
      "Activation Function: tanh\n",
      "Pooling Function: global_mean_pool\n",
      "Learning Rate: 0.01\n",
      "Batch Size: 32\n",
      "Weight Decay: 1e-05\n",
      "Epoch 0/50 => Test Loss: 2.2758, Test Accuracy: 0.1242\n",
      "Epoch 10/50 => Train Loss: 0.6283, Train Accuracy: 0.7987 | Test Loss: 0.8030, Test Accuracy: 0.6954\n",
      "Epoch 20/50 => Train Loss: 0.5034, Train Accuracy: 0.8321 | Test Loss: 0.7437, Test Accuracy: 0.7386\n",
      "Epoch 30/50 => Train Loss: 0.4194, Train Accuracy: 0.8580 | Test Loss: 0.7249, Test Accuracy: 0.7739\n",
      "Epoch 40/50 => Train Loss: 0.3847, Train Accuracy: 0.8668 | Test Loss: 0.6744, Test Accuracy: 0.7882\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for dimension 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mgenetic_algorithm_n_gens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpersons\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfitness_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfitness_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_gens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparent_selection_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtournament_fitness_based_selection\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[22], line 37\u001b[0m, in \u001b[0;36mgenetic_algorithm_n_gens\u001b[1;34m(population, fitness_function, parent_selection_function, survival_function, crossover_function, mutation_function, mutation_rate, allow_parents, n_gens, plot, display_last_gen)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGen \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     36\u001b[0m new_population \u001b[38;5;241m=\u001b[39m parent_selection_function(new_population, fitness_function, crossover_function, mutation_function, mutation_rate, allow_parents)\n\u001b[1;32m---> 37\u001b[0m new_population \u001b[38;5;241m=\u001b[39m \u001b[43msurvival_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_population\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfitness_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m fitness_scores \u001b[38;5;241m=\u001b[39m [fitness_function(individual) \u001b[38;5;28;01mfor\u001b[39;00m individual \u001b[38;5;129;01min\u001b[39;00m new_population]\n\u001b[0;32m     39\u001b[0m best_scores \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;28msorted\u001b[39m(fitness_scores)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]]\n",
      "Cell \u001b[1;32mIn[22], line 5\u001b[0m, in \u001b[0;36mtruncation\u001b[1;34m(population, fitness_function, npop)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtruncation\u001b[39m(population,fitness_function, npop):\n\u001b[1;32m----> 5\u001b[0m     fitness_values \u001b[38;5;241m=\u001b[39m [\u001b[43mfitness_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindividual\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m individual \u001b[38;5;129;01min\u001b[39;00m population]\n\u001b[0;32m      6\u001b[0m     indexes \u001b[38;5;241m=\u001b[39m argsort(fitness_values)[:npop]\n\u001b[0;32m      7\u001b[0m     selected_population \u001b[38;5;241m=\u001b[39m [population[index] \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m indexes]\n",
      "Cell \u001b[1;32mIn[23], line 263\u001b[0m, in \u001b[0;36mfitness_function\u001b[1;34m(person, epochs)\u001b[0m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;66;03m# Train the model and capture results\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 263\u001b[0m     train_acc, train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    264\u001b[0m     test_acc, test_loss \u001b[38;5;241m=\u001b[39m evaluate(model, test_loader, criterion, device)\n\u001b[0;32m    266\u001b[0m     train_accuracies\u001b[38;5;241m.\u001b[39mappend(train_acc)\n",
      "Cell \u001b[1;32mIn[23], line 165\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, optimizer, criterion, device)\u001b[0m\n\u001b[0;32m    163\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device) \n\u001b[0;32m    164\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 165\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, data\u001b[38;5;241m.\u001b[39my)\n\u001b[0;32m    167\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\Adeel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Adeel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[23], line 97\u001b[0m, in \u001b[0;36mGNNModelEA3.forward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     94\u001b[0m x, edge_index, batch \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mx, data\u001b[38;5;241m.\u001b[39medge_index, data\u001b[38;5;241m.\u001b[39mbatch\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# Apply the first graph convolutional layer followed by ReLU activation\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfirst\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_func(x)\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# Apply the second graph convolutional layer followed by ReLU activation\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Adeel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Adeel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Adeel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch_geometric\\nn\\conv\\gcn_conv.py:241\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[1;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[0;32m    239\u001b[0m cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_edge_index\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 241\u001b[0m     edge_index, edge_weight \u001b[38;5;241m=\u001b[39m \u001b[43mgcn_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# yapf: disable\u001b[39;49;00m\n\u001b[0;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimproved\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_self_loops\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcached:\n\u001b[0;32m    245\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_edge_index \u001b[38;5;241m=\u001b[39m (edge_index, edge_weight)\n",
      "File \u001b[1;32mc:\\Users\\Adeel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch_geometric\\nn\\conv\\gcn_conv.py:99\u001b[0m, in \u001b[0;36mgcn_norm\u001b[1;34m(edge_index, edge_weight, num_nodes, improved, add_self_loops, flow, dtype)\u001b[0m\n\u001b[0;32m     96\u001b[0m num_nodes \u001b[38;5;241m=\u001b[39m maybe_num_nodes(edge_index, num_nodes)\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m add_self_loops:\n\u001b[1;32m---> 99\u001b[0m     edge_index, edge_weight \u001b[38;5;241m=\u001b[39m \u001b[43madd_remaining_self_loops\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m edge_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    103\u001b[0m     edge_weight \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones((edge_index\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), ), dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m    104\u001b[0m                              device\u001b[38;5;241m=\u001b[39medge_index\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\Adeel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch_geometric\\utils\\loop.py:623\u001b[0m, in \u001b[0;36madd_remaining_self_loops\u001b[1;34m(edge_index, edge_attr, fill_value, num_nodes)\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Adds remaining self-loop :math:`(i,i) \\in \\mathcal{E}` to every node\u001b[39;00m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;124;03m:math:`i \\in \\mathcal{V}` in the graph given by :attr:`edge_index`.\u001b[39;00m\n\u001b[0;32m    592\u001b[0m \u001b[38;5;124;03mIn case the graph is weighted or has multi-dimensional edge features\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    620\u001b[0m \u001b[38;5;124;03m    tensor([0.5000, 0.5000, 1.0000, 1.0000]))\u001b[39;00m\n\u001b[0;32m    621\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    622\u001b[0m N \u001b[38;5;241m=\u001b[39m maybe_num_nodes(edge_index, num_nodes)\n\u001b[1;32m--> 623\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[43medge_index\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m!=\u001b[39m edge_index[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    625\u001b[0m device \u001b[38;5;241m=\u001b[39m edge_index\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m    626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for dimension 0 with size 0"
     ]
    }
   ],
   "source": [
    "genetic_algorithm_n_gens(persons, fitness_function=fitness_function, n_gens=20, parent_selection_function=tournament_fitness_based_selection)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
